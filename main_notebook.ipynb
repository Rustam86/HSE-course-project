{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rustam86/HSE-course-project/blob/main/main_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvX_8y90vtQT"
      },
      "source": [
        "#Настройка окружения\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtV8J4qcEIWj"
      },
      "outputs": [],
      "source": [
        "!sudo apt-get install unrar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAh-N1MyIulX"
      },
      "outputs": [],
      "source": [
        "!pip install biopython\n",
        "!pip install tqdm\n",
        "!pip install transformers\n",
        "!pip install ete3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "771lmy0mPelg"
      },
      "outputs": [],
      "source": [
        "# \"Install\" ZDNABERT\n",
        "%cd /content\n",
        "model = 'HG chipseq'\n",
        "model_id = '1VAsp8I904y_J0PUhAQqpSlCn1IqfG0FB'\n",
        "\n",
        "!gdown $model_id\n",
        "!gdown 10sF8Ywktd96HqAL0CwvlZZUUGj05CGk5\n",
        "!gdown 16bT7HDv71aRwyh3gBUbKwign1mtyLD2d\n",
        "!gdown 1EE9goZ2JRSD8UTx501q71lGCk-CK3kqG\n",
        "!gdown 1gZZdtAoDnDiLQqjQfGyuwt268Pe5sXW0\n",
        "\n",
        "\n",
        "!mkdir 6-new-12w-0\n",
        "!mv pytorch_model.bin 6-new-12w-0/\n",
        "!mv config.json 6-new-12w-0/\n",
        "!mv special_tokens_map.json 6-new-12w-0/\n",
        "!mv tokenizer_config.json 6-new-12w-0/\n",
        "!mv vocab.txt 6-new-12w-0/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oee8Up72R50C"
      },
      "outputs": [],
      "source": [
        "# \"Install\" ZHUNT\n",
        "!wget https://raw.githubusercontent.com/Rustam86/HSE-course-project/main/zhunt3-alan.c\n",
        "!gcc zhunt3-alan.c -lm -o zhunt3\n",
        "!chmod +x /content/zhunt3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ptaB8mZhRnRb"
      },
      "outputs": [],
      "source": [
        "# Install MAFFT\n",
        "!apt-get install mafft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ctRdkjFpve4U"
      },
      "outputs": [],
      "source": [
        "!mafft --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agTeHpIjvhAj"
      },
      "outputs": [],
      "source": [
        "# Install TrimAl\n",
        "!git clone https://github.com/scapella/trimal.git\n",
        "%cd trimal/source\n",
        "!make"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_uEqxsPwWbO"
      },
      "outputs": [],
      "source": [
        "!./trimal --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZyJqGeXbxWCZ"
      },
      "outputs": [],
      "source": [
        "# Install IQ-TREE (takes some time)\n",
        "%cd /content\n",
        "!apt-get install -y libeigen3-dev\n",
        "!git clone https://github.com/Cibiv/IQ-TREE.git\n",
        "%cd IQ-TREE\n",
        "!mkdir build\n",
        "%cd build\n",
        "!cmake ..\n",
        "!make -j4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlI0XN-_xhon"
      },
      "outputs": [],
      "source": [
        "!./iqtree --version"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIqZ5Pg7ewax"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import logging\n",
        "import os\n",
        "import random\n",
        "import sys\n",
        "import subprocess\n",
        "import tempfile\n",
        "import time\n",
        "from typing import Dict, List, Tuple, Callable, Optional, Union\n",
        "import warnings\n",
        "\n",
        "from ete3 import ClusterTree\n",
        "import colorsys\n",
        "from google.colab import files\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "import matplotlib as mpl\n",
        "import matplotlib.colors as mcolors\n",
        "from matplotlib.lines import Line2D\n",
        "from matplotlib.patches import Patch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas.api.types import CategoricalDtype\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import Bio\n",
        "from Bio import Entrez, SeqIO\n",
        "from Bio.SeqUtils import gc_fraction, GC_skew, MeltingTemp as mt\n",
        "from scipy.spatial.distance import squareform\n",
        "from scipy.cluster.hierarchy import dendrogram, fcluster, linkage, is_valid_linkage\n",
        "from scipy import ndimage, stats\n",
        "import sklearn\n",
        "from sklearn.linear_model import LinearRegression\n",
        "import torch\n",
        "from torch import nn\n",
        "import tqdm as tq\n",
        "from tqdm import tqdm\n",
        "import transformers\n",
        "from transformers import BertForTokenClassification, BertTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9XQbj6Dog92"
      },
      "outputs": [],
      "source": [
        "# Image resolution set to 300 dpi\n",
        "plt.rcParams[\"figure.dpi\"] = 300"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Y7Ke-0OllzN"
      },
      "outputs": [],
      "source": [
        "# Printing versions\n",
        "print(' '.join(f\"Python version: {sys.version}\".split()[:3]))\n",
        "print(f\"Matplotlib version: {matplotlib.__version__}\")\n",
        "print(f\"Numpy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n",
        "print(f\"Scipy version: {scipy.__version__}\")\n",
        "print(f\"Sklearn version: {sklearn.__version__}\")\n",
        "print(f\"Torch version: {torch.__version__}\")\n",
        "print(f\"Seaborn version: {sns.__version__}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Biopython version: {Bio.__version__}\")\n",
        "print(f\"TQDM version: {tq.__version__}\")\n",
        "print(f\"Transformers version: {transformers.__version__}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n5KCg8eVoUi8"
      },
      "source": [
        "## Предварительно подготовленные данные"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKFVtKwfN3kZ"
      },
      "outputs": [],
      "source": [
        "# Get file with Coronaviridae taxonomy information\n",
        "!wget https://raw.githubusercontent.com/Rustam86/HSE-course-project/main/Coronaviridae_taxonomic_info.csv\n",
        "# Read the taxonomic data\n",
        "taxa_db = pd.read_csv(\"/content/Coronaviridae_taxonomic_info.csv\", encoding='ISO-8859-1', delimiter=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQfUsJ4jpr2t"
      },
      "outputs": [],
      "source": [
        "# Dowload cornaviridae strains genbank files\n",
        "!wget https://github.com/Rustam86/HSE-course-project/raw/main/species_gb_files.rar\n",
        "!mkdir species_gb_files\n",
        "!unrar x /content/species_gb_files.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rVCsSoHKqSzP"
      },
      "outputs": [],
      "source": [
        "# Download json prediction files from github\n",
        "!wget https://github.com/Rustam86/HSE-course-project/raw/main/Coronaviridae_strains_json.rar\n",
        "!unrar x /content/Coronaviridae_strains_json.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rycPOirFZ1bF"
      },
      "outputs": [],
      "source": [
        "# Download clusterd dataframe csv files from github\n",
        "!wget https://github.com/Rustam86/HSE-course-project/raw/main/clustered_data_frames_dnabert.rar\n",
        "!unrar x clustered_data_frames_dnabert.rar\n",
        "\n",
        "!wget https://github.com/Rustam86/HSE-course-project/raw/main/clustered_data_frames_zhunt.rar\n",
        "!unrar x clustered_data_frames_zhunt.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsJXAmQN0F_e"
      },
      "outputs": [],
      "source": [
        "# Download row linkage matrices files from github\n",
        "!wget https://github.com/Rustam86/HSE-course-project/raw/main/row_linkage_matrices_dnabert.rar\n",
        "!unrar x row_linkage_matrices_dnabert.rar\n",
        "\n",
        "!wget https://github.com/Rustam86/HSE-course-project/raw/main/row_linkage_matrices_zhunt.rar\n",
        "!unrar x row_linkage_matrices_zhunt.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOB_ouSH8vvf"
      },
      "outputs": [],
      "source": [
        "# Download SARS-CoV-2 10k data from github\n",
        "!wget https://github.com/Rustam86/HSE-course-project/raw/main/SARS-CoV-2-parts.rar\n",
        "!unrar x SARS-CoV-2-parts.rar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5cx55xXjWtK"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Rustam86/HSE-course-project/main/meta_data_all_species.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Fho-hajRwP4"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/Rustam86/HSE-course-project/main/sars2_meta_10k.csv"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Rustam86/HSE-course-project/raw/main/SARS-CoV-2-full-genomes-only.rar\n",
        "!unrar x SARS-CoV-2-full-genomes-only.rar"
      ],
      "metadata": {
        "id": "ns4MrKB0R9fP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rcmRi4vb9swf"
      },
      "outputs": [],
      "source": [
        "!rm -r *.rar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZA0pEvxXrG9X"
      },
      "source": [
        "## Переменные окружения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qfDDnOYcrGIh"
      },
      "outputs": [],
      "source": [
        "# Set email (required for accessing NCBI databases via Entrez)\n",
        "Entrez.email = \"rustam@heydarov.ru\"\n",
        "\n",
        "ZDNABERT_PARAMS = {\n",
        "    'model_confidence_threshold': 0.2,\n",
        "    'minimum_sequence_length': 10,\n",
        "    'tokenizer': BertTokenizer.from_pretrained('/content/6-new-12w-0/'),\n",
        "    'model': BertForTokenClassification.from_pretrained('/content/6-new-12w-0/')\n",
        "}\n",
        "\n",
        "ZHUNT_PARAMS = {\n",
        "    'zhunt_path': '/content/zhunt3',\n",
        "    'score': 500,\n",
        "    'window_size': 6,\n",
        "    'min_size': 3,\n",
        "    'max_size': 6\n",
        "}\n",
        "\n",
        "colors = ['#003f5c', '#ffb5a7', '#9c88ff', '#ff7b00', '#955196', '#b5838d', '#edc7b7', '#6b705c',\n",
        "          '#f4d03f', '#d4af37', '#da627d', '#ff6e54', '#d1ccc0', '#7c7c7c', '#b7c0c7', '#c4aead',\n",
        "          '#8d99ae', '#444e86', '#9d8189', '#6497b1', '#d6e2e9', '#e5989b', '#006d77', '#95afc0',\n",
        "          '#dd5182', '#ffa600', '#5a189a', '#2e3440',  '#1f77b4', '#ff7f0e', '#2ca02c', '#d62728',\n",
        "          '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf', '#7c1e30', '#b29e7c',\n",
        "          '#2d2d87', '#beaed4', '#fdc086', '#bf5b17', '#f0027f', '#fbb4ae', '#fed9a6', '#b3cde3']\n",
        "\n",
        "\n",
        "PREVALENT_HOSTS = {\n",
        "    'Alphacoronavirus_1': 'Cats',\n",
        "    'Alphacoronavirus_AMALF': 'Bats',\n",
        "    'Alphacoronavirus_BT020': 'Bats',\n",
        "    'Alphacoronavirus_CHB25': 'Bats',\n",
        "    'Alphacoronavirus_HKU33': 'Bats',\n",
        "    'Alphacoronavirus_WA1087': 'Bats',\n",
        "    'Alphacoronavirus_WA2028': 'Bats',\n",
        "    'Alphacoronavirus_WA3607': 'Bats',\n",
        "    'Alphapironavirus_bona': 'Salmon',\n",
        "    'Avian_coronavirus': 'Chicken',\n",
        "    'Avian_coronavirus_9203': 'Chicken',\n",
        "    'Bat_Hp-betacoronavirus_Zhejiang2013': 'Bats',\n",
        "    'Bat_coronavirus_CDPHE15': 'Bats',\n",
        "    'Bat_coronavirus_HKU10': 'Bats',\n",
        "    'Beluga_whale_coronavirus_SW1': 'Whale',\n",
        "    'Betacoronavirus_1': 'Human/Swine/Dog',\n",
        "    'Bulbul_coronavirus_HKU11': 'Birds',\n",
        "    'China_Rattus_coronavirus_HKU24': 'Rat',\n",
        "    'Common_moorhen_coronavirus_HKU21': 'Birds',\n",
        "    'Coronavirus_HKU15': 'Swine',\n",
        "    'Duck_coronavirus_2714': 'Birds',\n",
        "    'Eidolon_bat_coronavirus_C704': 'Bats',\n",
        "    'Goose_coronavirus_CB17': 'Birds',\n",
        "    'Hedgehog_coronavirus_1': 'Hedgehog',\n",
        "    'Human_coronavirus_229E': 'Human',\n",
        "    'Human_coronavirus_HKU1': 'Human',\n",
        "    'Human_coronavirus_NL63': 'Human',\n",
        "    'Human_coronavirus_OC43': 'Human',\n",
        "    'Lucheng_Rn_rat_coronavirus': 'Rat',\n",
        "    'Middle_East_respiratory_syndrome-related_coronavirus': 'Human',\n",
        "    'Miniopterus_bat_coronavirus_1': 'Bats',\n",
        "    'Miniopterus_bat_coronavirus_HKU8': 'Bats',\n",
        "    'Mink_coronavirus_1': 'Mink',\n",
        "    'Munia_coronavirus_HKU13': 'Birds',\n",
        "    'Murine_coronavirus': 'Mice',\n",
        "    'Myodes_coronavirus_2JL14': 'Myodes',\n",
        "    'Myotis_ricketti_alphacoronavirus_Sax-2011': 'Bats',\n",
        "    'NL63-related_bat_coronavirus_strain_BtKYNL63-9b': 'Bats',\n",
        "    'Night_heron_coronavirus_HKU19': 'Birds',\n",
        "    'Nyctalus_velutinus_alphacoronavirus_SC-2013': 'Bats',\n",
        "    'Pipistrellus_bat_coronavirus_HKU5': 'Bats',\n",
        "    'Pipistrellus_kuhlii_coronavirus_3398': 'Bats',\n",
        "    'Porcine_epidemic_diarrhea_virus': 'Swine',\n",
        "    'Rhinolophus_bat_coronavirus_HKU2': 'Bats',\n",
        "    'Rhinolophus_ferrumequinum_alphacoronavirus_HuB-2013': 'Bats',\n",
        "    'Rousettus_bat_coronavirus_GCCDC1': 'Bats',\n",
        "    'Rousettus_bat_coronavirus_HKU9': 'Bats',\n",
        "    'Scotophilus_bat_coronavirus_512': 'Bats',\n",
        "    'Severe_acute_respiratory_syndrome_related_coronavirus': 'Human',\n",
        "    'Severe_acute_respiratory_syndrome_related_coronavirus_2': 'Human',\n",
        "    'Sorex_araneus_coronavirus_T14': 'Shrew',\n",
        "    'Suncus_murinus_coronavirus_X74': 'Shrew',\n",
        "    'Tylonycteris_bat_coronavirus_HKU4': 'Bats',\n",
        "    'White-eye_coronavirus_HKU16': 'Birds',\n",
        "    'Wigeon_coronavirus_HKU20': 'Birds'\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kTvKb_IRrOZw"
      },
      "source": [
        "## Вспомогательные функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k6qNb-X5rSc7"
      },
      "outputs": [],
      "source": [
        "class NumpyEncoder(json.JSONEncoder):\n",
        "    \"\"\"\n",
        "    Custom JSON encoder that converts numpy types to standard Python types.\n",
        "\n",
        "    Extends the standard JSON encoder class to handle numpy integer, floating-point,\n",
        "    and array types, converting them to standard Python types.\n",
        "    \"\"\"\n",
        "\n",
        "    def default(self, obj):\n",
        "        \"\"\"\n",
        "        Override the default method to handle numpy types.\n",
        "\n",
        "        Parameters:\n",
        "        obj: Object to encode.\n",
        "\n",
        "        Returns:\n",
        "        Encoded object.\n",
        "        \"\"\"\n",
        "        if isinstance(obj, np.integer):\n",
        "            return int(obj)\n",
        "        elif isinstance(obj, np.floating):\n",
        "            return float(obj)\n",
        "        elif isinstance(obj, np.ndarray):\n",
        "            return obj.tolist()\n",
        "        else:\n",
        "            return super(NumpyEncoder, self).default(obj)\n",
        "\n",
        "def sum_intervals(intervals: List[Tuple[int, int]]) -> int:\n",
        "    \"\"\"\n",
        "    Calculate the total sum of the lengths of given intervals, accounting for overlaps.\n",
        "\n",
        "    Parameters:\n",
        "    intervals (List[Tuple[int, int]]): List of intervals represented as tuples of start and end.\n",
        "\n",
        "    Returns:\n",
        "    int: Total length of all intervals, adjusted for overlaps. Returns 0 if the input list is empty.\n",
        "    \"\"\"\n",
        "    if not intervals:\n",
        "        return 0\n",
        "\n",
        "    # Sort intervals based on start times\n",
        "    intervals.sort(key=lambda x: x[0])\n",
        "\n",
        "    merged = [intervals[0]]\n",
        "    for current in intervals:\n",
        "        last = merged[-1]\n",
        "        if current[0] <= last[1]:\n",
        "            merged[-1] = (last[0], max(last[1], current[1]))\n",
        "        else:\n",
        "            merged.append(current)\n",
        "\n",
        "    return sum([end - start for start, end in merged])\n",
        "\n",
        "def save_json(data: dict, filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Save data to a JSON file.\n",
        "\n",
        "    Parameters:\n",
        "    - data (dict): The Python dictionary to save.\n",
        "    - filename (str): The name of the file where the data should be saved.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    with open(filename, 'w') as file:\n",
        "        json.dump(data, file, indent=4, cls=NumpyEncoder)\n",
        "\n",
        "def read_json_file(file_path: str) -> Optional[dict]:\n",
        "    try:\n",
        "        with open(file_path, 'r') as json_file:\n",
        "            data = json.load(json_file)\n",
        "        return data\n",
        "    except FileNotFoundError:\n",
        "        print(f\"File not found: {file_path}\")\n",
        "        return None\n",
        "    except PermissionError:\n",
        "        print(f\"Permission denied for file: {file_path}\")\n",
        "        return None\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"JSON decoding error: {e}\")\n",
        "        return None\n",
        "\n",
        "def list_files(directory: str) -> List[str]:\n",
        "    try:\n",
        "        return [os.path.join(directory, filename)\n",
        "                for filename in os.listdir(directory)\n",
        "                if os.path.isfile(os.path.join(directory, filename))]\n",
        "    except PermissionError:\n",
        "        print(f\"Permission denied for directory: {directory}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def datetime_to_ordinal(dt):\n",
        "    if pd.notna(dt):  # check if the value is not NaT\n",
        "        return dt.toordinal()\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def filter_arrays(arrays):\n",
        "    \"\"\"\n",
        "    Filters arrays by removing those that contain NaN values or have a length of zero.\n",
        "\n",
        "    Parameters:\n",
        "    - arrays (list): A list of arrays.\n",
        "\n",
        "    Returns:\n",
        "    - filtered_arrays (list): A list of filtered arrays.\n",
        "    \"\"\"\n",
        "    filtered_arrays = []\n",
        "    for array in arrays:\n",
        "        if np.isnan(array).any() or len(array) == 0:\n",
        "            continue\n",
        "        filtered_arrays.append(array)\n",
        "    return filtered_arrays\n",
        "\n",
        "def get_high_contrast_colors(n):\n",
        "    HSV_tuples = [(x*1.0/n, 0.8, 0.9) for x in range(n)]\n",
        "    RGB_tuples = map(lambda x: colorsys.hsv_to_rgb(*x), HSV_tuples)\n",
        "    hex_colors = [f'#{int(r*255):02x}{int(g*255):02x}{int(b*255):02x}' for r,g,b in RGB_tuples]\n",
        "    return hex_colors\n",
        "\n",
        "def generate_high_contrast_colors(n):\n",
        "    colors = []\n",
        "    for i in range(n):\n",
        "        # Cycle through hue\n",
        "        hue = float(i) / n\n",
        "        # Alternate between full and half brightness\n",
        "        lightness = 0.5 if i % 2 == 0 else 0.8\n",
        "        # Keep saturation constant\n",
        "        saturation = 0.9\n",
        "        # Convert to RGB\n",
        "        r, g, b = [int(x * 255) for x in colorsys.hls_to_rgb(hue, lightness, saturation)]\n",
        "        colors.append(f'#{r:02x}{g:02x}{b:02x}')\n",
        "    return colors\n",
        "\n",
        "def get_high_contrast_colors_gr(n):\n",
        "    phi = 0.618033988749895\n",
        "    h = random.random()  # random start value\n",
        "    colors = []\n",
        "    for _ in range(n):\n",
        "        h += phi\n",
        "        h %= 1\n",
        "        colors.append(mcolors.hsv_to_rgb([h, 1, 1]))\n",
        "    return colors\n",
        "\n",
        "unique_values = [...]  # Your list of unique values\n",
        "colors = get_high_contrast_colors(len(unique_values))\n",
        "color_map = dict(zip(unique_values, colors))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3NJtaRqrTCa"
      },
      "source": [
        "## Основные функции"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgBMXrBkrWgN"
      },
      "outputs": [],
      "source": [
        "def fetch_gb_files(accession_list: List[str], email: str, output_dir: str = \".\") -> None:\n",
        "    \"\"\"\n",
        "    Fetch GB (GenBank) files for a list of accession numbers.\n",
        "\n",
        "    Parameters:\n",
        "    accession_list (List[str]): List of accession numbers.\n",
        "    email (str): Your email address. NCBI requires this to track usage.\n",
        "    output_dir (str): Directory to save the GB files. Default is the current directory.\n",
        "\n",
        "    Returns:\n",
        "    None. GB files are saved to the specified directory.\n",
        "    \"\"\"\n",
        "\n",
        "    # Set email for Entrez\n",
        "    Entrez.email = email\n",
        "\n",
        "    # Iterate through the list of accession numbers\n",
        "    for accession in accession_list:\n",
        "        # Fetch the data for the given accession number\n",
        "        handle = Entrez.efetch(db=\"nucleotide\", id=accession, rettype=\"gb\", retmode=\"text\")\n",
        "\n",
        "        # Form the filename where the data will be saved\n",
        "        filename = f\"{output_dir}/{accession}.gb\"\n",
        "\n",
        "        # Write the data to the file\n",
        "        with open(filename, \"w\") as f:\n",
        "            f.write(handle.read())\n",
        "\n",
        "        # Close the handle\n",
        "        handle.close()\n",
        "\n",
        "        print(f\"{accession}.gb saved to {output_dir}\")\n",
        "\n",
        "def list_proteins(gb_file: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    List all protein names from a GB (GenBank) file.\n",
        "\n",
        "    Parameters:\n",
        "    gb_file (str): Path to the GenBank file.\n",
        "\n",
        "    Returns:\n",
        "    List[str]: List of protein names.\n",
        "    \"\"\"\n",
        "\n",
        "    protein_names = []\n",
        "\n",
        "    # Parse the GenBank file\n",
        "    for record in SeqIO.parse(gb_file, \"genbank\"):\n",
        "        for feature in record.features:\n",
        "            # Check if the feature is a coding sequence (CDS) and has a product\n",
        "            if feature.type == \"CDS\" and \"product\" in feature.qualifiers:\n",
        "                protein_names.append(feature.qualifiers[\"product\"][0])\n",
        "\n",
        "    return protein_names\n",
        "\n",
        "def extract_protein_sequence(gb_file: str, protein_name: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Extract amino acid sequence from a GB (GenBank) file given the name of a protein.\n",
        "\n",
        "    Parameters:\n",
        "    gb_file (str): Path to the GenBank file.\n",
        "    protein_name (str): Name of the protein to extract the sequence for.\n",
        "\n",
        "    Returns:\n",
        "    str, None: Amino acid sequence as a string if found, else None.\n",
        "    \"\"\"\n",
        "\n",
        "    # Parse the GenBank file\n",
        "    for record in SeqIO.parse(gb_file, \"genbank\"):\n",
        "        for feature in record.features:\n",
        "            if feature.type == \"CDS\":\n",
        "                # Check if the protein product name matches the given name\n",
        "                if \"product\" in feature.qualifiers and feature.qualifiers[\"product\"][0] == protein_name:\n",
        "                    # Extract the protein sequence\n",
        "                    return feature.qualifiers[\"translation\"][0]\n",
        "\n",
        "    return None\n",
        "\n",
        "def write_to_fasta(seq_names: List[str], sequences: List[str], filename: str) -> None:\n",
        "    \"\"\"\n",
        "    Write sequences to a FASTA file.\n",
        "\n",
        "    Parameters:\n",
        "    seq_names (List[str]): List of sequence names.\n",
        "    sequences (List[str]): List of corresponding sequences.\n",
        "    filename (str): Name of the output FASTA file.\n",
        "\n",
        "    Returns:\n",
        "    None. Writes the sequences to the specified FASTA file.\n",
        "    \"\"\"\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        for name, seq in zip(seq_names, sequences):\n",
        "            # Write the name and sequence to the file in FASTA format\n",
        "            f.write(f\">{name}\\n{seq}\\n\")\n",
        "\n",
        "# Generic function to run sequence processing functions on a batch of sequences\n",
        "def process_fasta_with_function(fasta_path: str, function: Callable, params: Dict) -> Dict[str, List[Tuple[int, int]]]:\n",
        "    \"\"\"\n",
        "    Run a function on a multi-fasta file.\n",
        "\n",
        "    Parameters:\n",
        "    fasta_path (str): The path to the fasta file.\n",
        "    function (Callable): The function to run on each sequence.\n",
        "    params (Dict): A dictionary containing the parameters for the function.\n",
        "\n",
        "    Returns:\n",
        "    Dict[str, List[Tuple[int, int]]]: A dictionary where the keys are the sequence\n",
        "                                      identifiers and the values are the results of\n",
        "                                      the function for each sequence.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    # Load all the sequences into a list to compute the total number of sequences\n",
        "    sequences = list(SeqIO.parse(fasta_path, \"fasta\"))\n",
        "\n",
        "    # Wrap the sequences list with tqdm for progress bar\n",
        "    with tqdm(sequences, desc=\"Processing\", unit=\"sequence\") as pbar:\n",
        "        for record in pbar:\n",
        "            sequence_id = record.id\n",
        "            sequence = str(record.seq)\n",
        "            results[sequence_id] = function(sequence, **params)\n",
        "\n",
        "    return results\n",
        "\n",
        "# Function to predict Z-RNA regions with ZDNABERT\n",
        "def run_zdnabert(seq_string: str, model:\n",
        "                 torch.nn.Module, tokenizer: torch.nn.Module,\n",
        "                 model_confidence_threshold: float = 0.2,\n",
        "                 minimum_sequence_length: int = 10) -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Process a DNA or RNA sequence string using a given model and identify segments of significance.\n",
        "\n",
        "    Parameters:\n",
        "    - seq_string (str): The DNA or RNA sequence to be processed.\n",
        "    - model (torch.nn.Module): Pretrained model for sequence prediction.\n",
        "    - tokenizer (torch.nn.Module): Tokenizer to convert the sequence into model-compatible tokens.\n",
        "    - model_confidence_threshold (float, optional): Threshold for considering a segment significant. Defaults to 0.2.\n",
        "    - minimum_sequence_length (int, optional): Minimum length of a segment to consider. Defaults to 10.\n",
        "\n",
        "    Returns:\n",
        "    List[Tuple[int, int]]: A list of tuples representing the start and end positions\n",
        "    of the predicted Z-DNA forming regions.\n",
        "    \"\"\"\n",
        "\n",
        "    model.cuda()\n",
        "    # Convert the sequence to k-mers\n",
        "    k = 6\n",
        "    kmer_seq = [seq_string.upper()[x:x+k] for x in range(len(seq_string)+1-k)]\n",
        "\n",
        "    # Split the kmer sequence into pieces of a specified length with some overlap (padding)\n",
        "    length, pad = 512, 16\n",
        "    seq_pieces = [kmer_seq[st:min(st+512, len(kmer_seq))] for st in range(0, len(kmer_seq), length-pad)]\n",
        "\n",
        "    # Use the model to predict on each sequence piece\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for seq_piece in seq_pieces:\n",
        "            input_ids = torch.LongTensor(tokenizer.encode(' '.join(seq_piece), add_special_tokens=False))\n",
        "            outputs = torch.softmax(model(input_ids.cuda().unsqueeze(0))[-1], axis=-1)[0, :, 1]\n",
        "            preds.append(outputs.cpu().numpy())\n",
        "\n",
        "    # Stitch together the predictions for each piece of the sequence\n",
        "    res = np.array([])\n",
        "    for seq in preds:\n",
        "        res = np.concatenate([res[:-pad], seq])\n",
        "    stitched_seq = res\n",
        "\n",
        "    # Identify segments with prediction confidence above the threshold\n",
        "    out = []\n",
        "    labeled, max_label = scipy.ndimage.label(stitched_seq > model_confidence_threshold)\n",
        "    for label in range(1, max_label+1):\n",
        "        candidate = np.where(labeled == label)[0]\n",
        "        candidate_length = candidate.shape[0]\n",
        "        # Consider segments only if they are longer than the specified minimum sequence length\n",
        "        if candidate_length > minimum_sequence_length:\n",
        "            out.append((candidate[0], candidate[-1]))\n",
        "\n",
        "    return out\n",
        "\n",
        "# Function to predict Z-RNA regions with ZHUNT\n",
        "def run_zhunt(seq_string: str, zhunt_path: str, score: int = 500,\n",
        "              window_size: int = 6, min_size: int = 3,\n",
        "              max_size: int = 6) -> List[Tuple[int, int]]:\n",
        "    \"\"\"\n",
        "    Run the ZHunt program to predict Z-DNA forming regions in a DNA sequence.\n",
        "\n",
        "    Parameters:\n",
        "    - seq_string (str): The DNA or RNA sequence to be processed.\n",
        "    - zhunt_path (str): The path to the ZHunt executable.\n",
        "    - window_size (int): The window size for the ZHunt program. Default is 6.\n",
        "    - min_size (int): The minimum size for the ZHunt program. Default is 3.\n",
        "    - max_size (int): The maximum size for the ZHunt program. Default is 6.\n",
        "\n",
        "    Returns:\n",
        "    List[Tuple[int, int]]: A list of tuples representing the start and end positions\n",
        "    of the predicted Z-DNA forming regions.\n",
        "    \"\"\"\n",
        "    # Ensure the sequence only contains valid DNA bases\n",
        "    # assert set(sequence).issubset({\"A\", \"C\", \"G\", \"T\", \"N\"}), \"Invalid DNA sequence\"\n",
        "\n",
        "    # Create a temporary file\n",
        "    file_descriptor, temp_file_path = tempfile.mkstemp()\n",
        "    os.close(file_descriptor)\n",
        "\n",
        "    try:\n",
        "        # Write the sequence to the temporary file\n",
        "        with open(temp_file_path, 'w') as temp_file:\n",
        "            temp_file.write(seq_string)\n",
        "\n",
        "        # Run the ZHunt program\n",
        "        subprocess.run(\n",
        "            [zhunt_path, str(window_size), str(min_size), str(max_size), temp_file_path],\n",
        "            check=True, stdout=subprocess.PIPE, stderr=subprocess.DEVNULL,\n",
        "            input=seq_string, encoding='ascii'\n",
        "        )\n",
        "\n",
        "        # Read the ZHunt output into a DataFrame\n",
        "        with open(temp_file_path + \".Z-SCORE\", 'r') as zhunt_output:\n",
        "            output_data = pd.read_csv(zhunt_output,\n",
        "                             names=['Start', 'End', 'nu-1', 'nu-2', 'nu-3',\n",
        "                                    'ZH-Score', 'Sequence', 'Conformation'],\n",
        "                             skiprows=1, sep='\\s+')\n",
        "\n",
        "        # Filter the DataFrame to only include rows with a ZH-Score greater than 500\n",
        "        filtered_data = output_data[output_data['ZH-Score'] > score]\n",
        "\n",
        "        # Return a list of tuples representing the start and end positions of the predicted Z-DNA forming regions\n",
        "        return list(zip(filtered_data['Start'], filtered_data['End']))\n",
        "\n",
        "    except Exception as error:\n",
        "        print(f\"An error occurred while running ZHunt: {error}\")\n",
        "\n",
        "    finally:\n",
        "        # Clean up the temporary files\n",
        "        os.remove(temp_file_path)\n",
        "        os.remove(temp_file_path + \".Z-SCORE\")\n",
        "\n",
        "def get_taxid(species_name: str) -> Optional[str]:\n",
        "    \"\"\"\n",
        "    Retrieve the taxonomic identifier (taxid) for a given species name.\n",
        "\n",
        "    Args:\n",
        "        species_name (str): The name of the species for which the taxid is required.\n",
        "\n",
        "    Returns:\n",
        "        str: The taxid for the species, if found. None otherwise.\n",
        "    \"\"\"\n",
        "    handle = Entrez.esearch(db=\"taxonomy\", term=species_name)\n",
        "    record = Entrez.read(handle)\n",
        "    handle.close()\n",
        "    try:\n",
        "        return record[\"IdList\"][0]\n",
        "    except IndexError:\n",
        "        return None\n",
        "\n",
        "def fetch_genbank_taxid(taxid: str, filename: str, n: int) -> None:\n",
        "    \"\"\"\n",
        "    Download a fixed number of random genomic GenBank files for a given taxid and save to a file.\n",
        "\n",
        "    Args:\n",
        "        taxid (str): The taxid for which GenBank files are required.\n",
        "        filename (str): The name of the file to save the GenBank data.\n",
        "        n (int): The fixed number of GenBank files to fetch.\n",
        "\n",
        "    Returns:\n",
        "        None\n",
        "    \"\"\"\n",
        "    term = f'txid{taxid}[organism:exp] AND biomol_genomic[prop] AND (\"25000\"[SLEN] : \"35000\"[SLEN])'\n",
        "    handle = Entrez.esearch(db=\"nuccore\",\n",
        "                            term=term,\n",
        "                            retmax=10000000)\n",
        "    record = Entrez.read(handle)\n",
        "    id_list = record[\"IdList\"]\n",
        "\n",
        "    if not id_list:\n",
        "        print(f\"No GenBank records found for taxid {taxid}\")\n",
        "        return\n",
        "\n",
        "    # Randomly select n IDs from id_list\n",
        "    selected_ids = random.sample(id_list, min(n, len(id_list)))\n",
        "\n",
        "    # Using tqdm to show progress bar\n",
        "    with open(filename, \"w\") as out, tqdm(total=len(selected_ids), desc=\"Fetching GenBank records\") as pbar:\n",
        "        for gb_id in selected_ids:\n",
        "            handle = Entrez.efetch(db=\"nuccore\", id=gb_id, rettype=\"gb\", retmode=\"text\")\n",
        "            out.write(handle.read())\n",
        "            pbar.update(1)  # update progress bar for each GenBank record\n",
        "            time.sleep(0.35)  # Optional: to avoid hitting API rate limits\n",
        "\n",
        "def count_genbank_entries(taxid: str) -> int:\n",
        "    \"\"\"\n",
        "    Count the number of GenBank entries for a given taxid.\n",
        "\n",
        "    Args:\n",
        "        taxid (str): The taxid for which the GenBank entry count is required.\n",
        "\n",
        "    Returns:\n",
        "        int: The count of GenBank entries associated with the taxid.\n",
        "    \"\"\"\n",
        "    term = f'txid{taxid}[organism:exp] AND biomol_genomic[prop] AND (\"25000\"[SLEN] : \"35000\"[SLEN])'\n",
        "    handle = Entrez.esearch(db=\"nuccore\",\n",
        "                            term=term,\n",
        "                            retmax=100000000)\n",
        "    record = Entrez.read(handle)\n",
        "    return len(record[\"IdList\"])\n",
        "\n",
        "def newick_to_linkage(newick: str, label_order: list[str] = None) -> (np.ndarray, list[str]):\n",
        "    \"\"\"\n",
        "    Convert a Newick formatted tree into a linkage matrix and retrieve corresponding labels.\n",
        "\n",
        "    Parameters:\n",
        "    - newick (str): A Newick formatted string representation of the tree.\n",
        "    - label_order (list[str], optional): Desired order of labels in the output.\n",
        "                                         If not provided, the order from the Newick string is used.\n",
        "\n",
        "    Returns:\n",
        "    - np.ndarray: Linkage matrix representation of the tree.\n",
        "    - list[str]: List of labels corresponding to the tree nodes.\n",
        "\n",
        "    Raises:\n",
        "    - AssertionError: If there are labels in `label_order` which are not present in the Newick string.\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert Newick string to ClusterTree\n",
        "    tree = ClusterTree(newick)\n",
        "\n",
        "    # Get the cophenetic matrix and labels from the tree\n",
        "    cophenetic_matrix, newick_labels = tree.cophenetic_matrix()\n",
        "    cophenetic_matrix = pd.DataFrame(cophenetic_matrix, columns=newick_labels, index=newick_labels)\n",
        "\n",
        "    # If a label order is provided, reorder the cophenetic matrix rows and columns accordingly\n",
        "    if label_order is not None:\n",
        "        # Identify missing and superfluous labels\n",
        "        missing_labels = set(label_order).difference(set(newick_labels))\n",
        "        superfluous_labels = set(newick_labels).difference(set(label_order))\n",
        "\n",
        "        # Check for labels that are in `label_order` but not in Newick string\n",
        "        assert len(missing_labels) == 0, f'Some labels are not in the newick string: {missing_labels}'\n",
        "\n",
        "        # Warn if there are labels in the Newick string that are not used in `label_order`\n",
        "        if len(superfluous_labels) > 0:\n",
        "            logging.warning(f'Newick string contains unused labels: {superfluous_labels}')\n",
        "\n",
        "        # Reorder the cophenetic matrix\n",
        "        cophenetic_matrix = cophenetic_matrix.reindex(index=label_order, columns=label_order)\n",
        "\n",
        "    # Convert the cophenetic matrix to a pairwise distance matrix\n",
        "    pairwise_distances = squareform(cophenetic_matrix)\n",
        "\n",
        "    # Return linkage matrix and labels\n",
        "    return linkage(pairwise_distances), list(cophenetic_matrix.columns)\n",
        "\n",
        "def total_interval_length(intervals_dict: Dict[str, List[Tuple[int, int]]]) -> Dict[str, int]:\n",
        "    \"\"\"\n",
        "    Given a dictionary of intervals for each ID, this function calculates\n",
        "    the total length of the intervals for each ID.\n",
        "\n",
        "    Parameters:\n",
        "    intervals_dict (Dict[str, List[Tuple[int, int]]]): Dictionary where key is ID and value is a list of intervals.\n",
        "\n",
        "    Returns:\n",
        "    Dict[str, int]: Dictionary where key is ID and value is total length of intervals.\n",
        "    \"\"\"\n",
        "\n",
        "    length_dict = {}\n",
        "\n",
        "    for key, intervals in intervals_dict.items():\n",
        "        # Calculate total length for the current key by subtracting start from end for each interval\n",
        "        total_length = sum([end - start for start, end in intervals])\n",
        "        # Splitting the key and using the first part as the new key\n",
        "        length_dict[key.split('.')[0]] = total_length\n",
        "\n",
        "    return length_dict\n",
        "\n",
        "def plot_tree_with_annotations(newick: str, numeric_values: dict,\n",
        "                               taxa_db: pd.DataFrame, color_threshold: float = 0.9) -> None:\n",
        "    \"\"\"\n",
        "    Plot a tree with annotations using a Newick formatted string along with numeric values\n",
        "    represented as a bar plot beside the dendrogram.\n",
        "\n",
        "    Parameters:\n",
        "    - newick (str): A Newick formatted string representation of the tree.\n",
        "    - numeric_values (dict): A dictionary containing the numeric values corresponding to the labels. e.g. {\"label1\": 5.6}\n",
        "    - taxa_db (pd.DataFrame): A DataFrame containing 'Accession' and 'Prevalent host' data.\n",
        "    - color_threshold (float, optional): The threshold for coloring branches. Defaults to 0.9.\n",
        "\n",
        "    Returns:\n",
        "    - None\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert newick string to linkage matrix and get labels\n",
        "    linkage_matrix, labels = newick_to_linkage(newick)  # Ensure you've defined newick_to_linkage function\n",
        "\n",
        "    switch = False\n",
        "\n",
        "    if switch:\n",
        "      labels = [dict(zip((taxa_db['Accession']), taxa_db['Species']))[id] for id in labels]\n",
        "\n",
        "    # Create a triple subplot - one for the tree, one for numeric value bars, and one for the legend\n",
        "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(12, 6), gridspec_kw={'width_ratios': [2, 0.5, 0.5]})\n",
        "\n",
        "    # Plot tree on the left subplot\n",
        "    dendro_data = dendrogram(linkage_matrix, labels=labels, orientation='left', ax=ax1, color_threshold=color_threshold)\n",
        "\n",
        "    # Generate host-related color mapping from the DataFrame\n",
        "    host_groups = sorted(set(taxa_db['Prevalent host'].unique()))\n",
        "    id_to_host = dict(zip(taxa_db['Accession'], taxa_db['Prevalent host']))\n",
        "    palette = sns.color_palette(\"husl\", len(host_groups))\n",
        "    host_to_color = dict(zip(host_groups, palette))\n",
        "\n",
        "    # Update the color for each label according to its prevalent host\n",
        "    label_color_mapping = {}\n",
        "    for label in dendro_data['ivl']:\n",
        "        if switch:\n",
        "            label = dict(zip((taxa_db['Species']), taxa_db['Accession']))[label]\n",
        "        prevalent_host = id_to_host.get(label)\n",
        "        label_color_mapping[label] = host_to_color.get(prevalent_host, \"grey\")\n",
        "\n",
        "\n",
        "    # Plot tree on the left subplot\n",
        "    dendro_data = dendrogram(linkage_matrix, labels=labels, orientation='left', ax=ax1, color_threshold=color_threshold)\n",
        "\n",
        "    # Get the number of leaves (labels) in tree\n",
        "    num_leaves = len(dendro_data['ivl'])\n",
        "\n",
        "    # Get the maximum numeric value for x-limit of the middle subplot\n",
        "    max_width = max(numeric_values.values())\n",
        "\n",
        "    # Plot numeric values as colored boxes in the middle subplot\n",
        "    for label, y in zip(dendro_data['ivl'], range(num_leaves)):\n",
        "        if switch:\n",
        "            label = dict(zip((taxa_db['Species']), taxa_db['Accession']))[label]\n",
        "        box_width = numeric_values.get(label, 0)\n",
        "        color = label_color_mapping.get(label, \"grey\")\n",
        "        ax2.add_patch(plt.Rectangle((0, y*(ax1.get_ylim()[1]/num_leaves) + 0.25*ax1.get_ylim()[1]/num_leaves),\n",
        "                                    box_width, 8, facecolor=color))\n",
        "\n",
        "    # Set limits for the middle subplot and hide its y-axis\n",
        "    ax2.set_xlim(0, max_width)\n",
        "    ax2.set_ylim(ax1.get_ylim())\n",
        "    ax2.yaxis.set_visible(False)\n",
        "\n",
        "    # Hide x and y axes for the legend axis\n",
        "    ax3.axis('off')\n",
        "\n",
        "    # Populate the legend on the third axis\n",
        "    for i, host in enumerate(host_groups):\n",
        "        ax3.plot([], [], 'o', color=host_to_color[host], label=host)\n",
        "    ax3.legend(title='Prevalent Host', loc='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def compute_jaccard_index(set_intervals1: List[Tuple[int, int]], set_intervals2: List[Tuple[int, int]]) -> float:\n",
        "    \"\"\"\n",
        "    This function computes the Jaccard index between two sets of intervals.\n",
        "\n",
        "    Parameters:\n",
        "    - set_intervals1, set_intervals2 (list of tuples): The intervals to be compared.\n",
        "\n",
        "    Returns:\n",
        "    jaccard_index (float): The Jaccard index between the two sets of intervals.\n",
        "    \"\"\"\n",
        "\n",
        "    # Compute the intersection of the two sets of intervals\n",
        "    intersection_size = sum(min(interval2[1], interval1[1]) - max(interval2[0], interval1[0])\n",
        "                            for interval1 in set_intervals1\n",
        "                            for interval2 in set_intervals2\n",
        "                            if interval1[1] > interval2[0] and interval2[1] > interval1[0])\n",
        "\n",
        "    # Compute the size of intervals in set_intervals1 that do not overlap with set_intervals2\n",
        "    non_overlap_size1 = sum(interval1[1] - interval1[0]\n",
        "                            for interval1 in set_intervals1\n",
        "                            if not any(interval1[1] > interval2[0] and interval2[1] > interval1[0]\n",
        "                                       for interval2 in set_intervals2))\n",
        "\n",
        "    # Compute the size of intervals in set_intervals2 that do not overlap with set_intervals1\n",
        "    non_overlap_size2 = sum(interval2[1] - interval2[0]\n",
        "                            for interval2 in set_intervals2\n",
        "                            if not any(interval2[1] > interval1[0] and interval1[1] > interval2[0]\n",
        "                                       for interval1 in set_intervals1))\n",
        "\n",
        "    # Compute the size of the union of the two sets of intervals\n",
        "    union_size = intersection_size + non_overlap_size1 + non_overlap_size2\n",
        "\n",
        "    # Compute the Jaccard index\n",
        "    if union_size:\n",
        "        jaccard_index = intersection_size / union_size\n",
        "    else:\n",
        "        jaccard_index = 1\n",
        "\n",
        "    return jaccard_index\n",
        "\n",
        "\n",
        "def create_clustered_dataframe(zrna_intervals_dict: Dict) -> Tuple[pd.DataFrame, np.ndarray]:\n",
        "    \"\"\"\n",
        "    This function creates a clustered dataframe based on Jaccard indices of intervals.\n",
        "\n",
        "    Parameters:\n",
        "    - zrna_intervals_dict (Dict): The dictionary containing Z-RNA interval data.\n",
        "\n",
        "    Returns:\n",
        "    clustered_dataframe (pd.DataFrame): The clustered dataframe.\n",
        "    row_linkage_matrix (np.ndarray): The hierarchical clustering encoded as a linkage matrix.\n",
        "    \"\"\"\n",
        "\n",
        "    # Parse the intervals file\n",
        "    intervals_dict = zrna_intervals_dict\n",
        "\n",
        "    sequence_ids = list(intervals_dict.keys())\n",
        "\n",
        "    # Check if there are enough sequences\n",
        "    if len(sequence_ids) < 2:\n",
        "        return pd.DataFrame({list(zrna_intervals_dict.keys())[0]: 0},\n",
        "                            index=list(zrna_intervals_dict.keys()),\n",
        "                            columns=list(zrna_intervals_dict.keys())), None, None\n",
        "\n",
        "    # Compute the Jaccard index matrix with a single progress bar\n",
        "    print(\"Computing Jaccard index matrix...\")\n",
        "    intervals_list = list(intervals_dict.items())\n",
        "    interval_count = len(intervals_list)\n",
        "    sequence_labels = [intervals_list[i][0] for i in range(interval_count)]\n",
        "\n",
        "    jaccard_index_matrix = np.zeros((interval_count, interval_count))\n",
        "    with tqdm(total=interval_count**2, desc=\"Computing Jaccard indices\") as pbar:\n",
        "        for i in range(interval_count):\n",
        "            for j in range(interval_count):\n",
        "                jaccard_index_matrix[i][j] = compute_jaccard_index(intervals_list[i][1], intervals_list[j][1])\n",
        "                pbar.update(1)\n",
        "\n",
        "    jaccard_dataframe = pd.DataFrame(jaccard_index_matrix, index=sequence_labels, columns=sequence_labels)\n",
        "\n",
        "    # Perform hierarchical clustering on rows and columns\n",
        "    print(\"Performing hierarchical clustering on rows...\")\n",
        "    row_linkage_matrix = linkage(jaccard_dataframe.values, method='average', metric='euclidean')\n",
        "    print(\"Performing hierarchical clustering on columns...\")\n",
        "    column_linkage_matrix = linkage(jaccard_dataframe.values.T, method='average', metric='euclidean')\n",
        "\n",
        "    # Reorder the dataframe based on the clustering\n",
        "    print(\"Reordering dataframe based on clustering...\")\n",
        "    row_dendrogram = dendrogram(row_linkage_matrix, no_plot=True)\n",
        "    column_dendrogram = dendrogram(column_linkage_matrix, no_plot=True)\n",
        "    clustered_dataframe = jaccard_dataframe.iloc[row_dendrogram['leaves'], column_dendrogram['leaves']]\n",
        "\n",
        "    return clustered_dataframe, row_linkage_matrix\n",
        "\n",
        "def extract_regions_from_genbank_id(genbank_id: str) -> List[Tuple[str, int, int]]:\n",
        "    \"\"\"\n",
        "    Extract genomic regions including CDS (excluding \"ORF1ab polyprotein\" and \"ORF1a polyprotein\"),\n",
        "    mat_peptides, and UTRs from a given GenBank ID.\n",
        "\n",
        "    Parameters:\n",
        "    - genbank_id (str): The GenBank ID to fetch.\n",
        "\n",
        "    Returns:\n",
        "    - List[Tuple[str, int, int]]: A list of tuples where each tuple contains:\n",
        "      * The product name or region type (str).\n",
        "      * The start coordinate (int).\n",
        "      * The end coordinate (int).\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    # Fetch the GenBank record\n",
        "    handle = Entrez.efetch(db=\"nucleotide\", id=genbank_id, rettype=\"gb\", retmode=\"text\")\n",
        "\n",
        "    regions = []\n",
        "\n",
        "    for record in SeqIO.parse(handle, \"genbank\"):\n",
        "        for feature in record.features:\n",
        "\n",
        "            # Extract CDS coordinates and product names\n",
        "            if feature.type == \"CDS\":\n",
        "                product = feature.qualifiers.get('product', ['unknown'])[0]\n",
        "                if product not in [\"ORF1ab polyprotein\", \"ORF1a polyprotein\"]:\n",
        "                    start = int(feature.location.start) + 1\n",
        "                    end = int(feature.location.end)\n",
        "                    regions.append((product, start, end))\n",
        "\n",
        "            # Extract mat_peptide coordinates and product names\n",
        "            elif feature.type == \"mat_peptide\":\n",
        "                start = int(feature.location.start) + 1\n",
        "                end = int(feature.location.end)\n",
        "                product = feature.qualifiers.get('product', ['unknown'])[0]\n",
        "                regions.append((product, start, end))\n",
        "\n",
        "            # Extract UTR coordinates\n",
        "            elif feature.type == \"5'UTR\" or feature.type == \"3'UTR\":\n",
        "                start = int(feature.location.start) + 1\n",
        "                end = int(feature.location.end)\n",
        "                regions.append((feature.type, start, end))\n",
        "\n",
        "    handle.close()\n",
        "\n",
        "    # Remove duplicates and sort regions based on start coordinate\n",
        "    regions = list(set(regions))\n",
        "    regions.sort(key=lambda x: x[1])\n",
        "\n",
        "    return regions\n",
        "\n",
        "def plot_zna_regions(virus_name: str, sorted_keys: List[str],\n",
        "                     regions_1: Dict[str, List[Tuple[int, int]]],\n",
        "                     regions_2: Dict[str, List[Tuple[int, int]]],\n",
        "                     genomic_regions_data: List[Tuple[str, int, int]],\n",
        "                     colors: List[str],\n",
        "                     figsize: Tuple[int, int] = (30, 15),\n",
        "                     save: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Plot Z-DNA regions predicted by two different methods, Z-HUNT and Z-DNABERT,\n",
        "    across different strains of a virus species.\n",
        "\n",
        "    Args:\n",
        "    - virus_name (str): The name of the virus.\n",
        "    - sorted_keys (list): A list of keys (strains names) used to order the data.\n",
        "    - regions_1 (dict): A dictionary containing the Z-DNA regions predicted by Z-HUNT.\n",
        "                      Each key is a strain name and each value is a list of start\n",
        "                      and end positions of the Z-DNA regions.\n",
        "    - regions_2 (dict): A dictionary containing the Z-DNA regions predicted by Z-DNABERT.\n",
        "                      Structured the same as regions_1.\n",
        "    - genomic_regions_data (list): A list of tuples where each tuple contains the name of a gene and its\n",
        "                 start and end positions.\n",
        "    - colors (list): A list of colors to use for the horizontal lines representing genes.\n",
        "    - figsize (tuple, optional): The size of the figure to plot. Defaults to (30, 15).\n",
        "    - save (bool, optional): If True, save the plot in PNG and PDF formats. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "    None. Shows the plot inline or saves it to file.\n",
        "    \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "    genes, starts, ends = zip(*genomic_regions_data)\n",
        "\n",
        "    for i in range(len(genomic_regions_data)):\n",
        "        ax.hlines(y=0, xmin=starts[i], xmax=ends[i], linewidth=10, color=colors[i])\n",
        "\n",
        "    lim = max(ends) + 50\n",
        "    ax.set_xlim(0, lim)\n",
        "\n",
        "    ax.yaxis.set_visible(False)\n",
        "    ax.grid(axis='x', linestyle='--', alpha=0.7)\n",
        "    ax.spines['top'].set_visible(False)\n",
        "    ax.spines['right'].set_visible(False)\n",
        "\n",
        "    ax.spines['bottom'].set_linewidth(1.5)\n",
        "    ax.spines['bottom'].set_position(('outward', 10))\n",
        "\n",
        "    xticks = []\n",
        "    xlabels = []\n",
        "    for i in range(len(genomic_regions_data)):\n",
        "        xticks.append((starts[i] + ends[i]) / 2)\n",
        "        xlabels.append(genomic_regions_data[i][0])\n",
        "    ax.set_xticks(xticks)\n",
        "    ax.set_xticklabels(xlabels, fontsize=10)\n",
        "\n",
        "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
        "\n",
        "    patches = [plt.Rectangle((0, 0), 1, 1, color=colors[i]) for i in range(len(genomic_regions_data))]\n",
        "    ax.legend(patches, genes, loc='upper center', bbox_to_anchor=(0.5, -0.25), ncol=5, fontsize=10)\n",
        "\n",
        "    plt.subplots_adjust(bottom=0.35)\n",
        "\n",
        "    counter = 1\n",
        "    for key in sorted_keys:\n",
        "        if key in regions_1.keys():\n",
        "            adata = [(regions_1[key][1:][i][0], regions_1[key][1:][i][1]) for i in range(len(regions_1[key][1:]))]\n",
        "            if adata:\n",
        "              starts, ends = zip(*adata)\n",
        "              for i in range(len(adata)):\n",
        "                  ax.hlines(y=counter, xmin=starts[i], xmax=ends[i], linewidth=5, colors='#e8071a', label='Z-HUNT')\n",
        "              counter += 0.25\n",
        "\n",
        "    counter = 1\n",
        "    for key in sorted_keys:\n",
        "        if key in regions_2.keys():\n",
        "            adata = [(regions_2[key][1:][i][0], regions_2[key][1:][i][1]) for i in range(len(regions_2[key][1:]))]\n",
        "            if adata:\n",
        "              starts, ends = zip(*adata)\n",
        "              for i in range(len(adata)):\n",
        "                  ax.hlines(y=counter, xmin=starts[i], xmax=ends[i], linewidth=5, colors='#07a1e8', label='Z-DNABERT')\n",
        "              counter += 0.25\n",
        "\n",
        "    n_strains = len(regions_1)\n",
        "    title = f\"{virus_name}\\n\\n(number of strains: {n_strains})\"\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    ax.set_xlabel('Position (nucleotides)', fontsize=12)\n",
        "\n",
        "    if save:\n",
        "        fig.savefig(f\"{virus_name}.png\", bbox_inches='tight')\n",
        "        fig.savefig(f\"{virus_name}.pdf\", bbox_inches='tight')\n",
        "    else:\n",
        "        plt.show()\n",
        "\n",
        "def extract_genbank_data(file_list: list) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Extracts various data and calculations from a list of GenBank files.\n",
        "\n",
        "    Parameters:\n",
        "    - file_list (list): A list of paths to GenBank files.\n",
        "\n",
        "    Returns:\n",
        "    - pd.DataFrame: A DataFrame containing the extracted details for each GenBank file.\n",
        "    \"\"\"\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for file_path in file_list:\n",
        "        with open(file_path, \"r\") as handle:\n",
        "            for record in SeqIO.parse(handle, \"genbank\"):\n",
        "                # General Details\n",
        "                accession_with_version = record.id\n",
        "                virus_species_from_file = file_path.split(\"/\")[-1].split(\".\")[0]\n",
        "\n",
        "                # Taxonomy Details\n",
        "                taxonomy = record.annotations.get('taxonomy', [])\n",
        "                subfamily = taxonomy[8] if len(taxonomy) >= 9 else \"\"\n",
        "                genus = taxonomy[9] if len(taxonomy) >= 10 else \"\"\n",
        "                subgenus = taxonomy[10] if len(taxonomy) >= 11 else \"\"\n",
        "                species = taxonomy[11] if len(taxonomy) >= 12 else \"\"\n",
        "\n",
        "                # Source Features\n",
        "                host = None\n",
        "                collection_date = None\n",
        "                for feature in record.features:\n",
        "                    if feature.type == \"source\":\n",
        "                        host = feature.qualifiers.get(\"host\", [None])[0]\n",
        "                        collection_date = feature.qualifiers.get(\"collection_date\", [None])[0]\n",
        "                        break\n",
        "                if not collection_date:\n",
        "                    collection_date = record.annotations.get('date', '')\n",
        "\n",
        "                # Sequence Calculations\n",
        "                seq_length = len(record.seq)\n",
        "                gc_content = gc_fraction(record.seq) * 100\n",
        "                melting_temperature = mt.Tm_NN(record.seq)\n",
        "\n",
        "                gc_skew_values = GC_skew(record.seq, window=100)\n",
        "                gc_skew_avg = sum(gc_skew_values) / len(gc_skew_values) if gc_skew_values else 0\n",
        "\n",
        "                data.append([\n",
        "                    virus_species_from_file, accession_with_version, subfamily,\n",
        "                    genus, subgenus, species, host, seq_length, gc_content,\n",
        "                    melting_temperature, gc_skew_avg,\n",
        "                    collection_date\n",
        "                ])\n",
        "\n",
        "    columns = [\n",
        "        'virus_species_from_file', 'accession_with_version', 'subfamily',\n",
        "        'genus', 'subgenus', 'species', 'host', 'Sequence length', 'GC content',\n",
        "        'Melting temperature', 'GC skew', 'date'\n",
        "    ]\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "    df['datetime'] = pd.to_datetime(df['date'], errors='coerce')\n",
        "\n",
        "    return df\n",
        "\n",
        "def plot_heatmap_with_dendrogram(clustered_dataframe: pd.DataFrame,\n",
        "                                 row_linkage_matrix: np.ndarray,\n",
        "                                 color_threshold: float,\n",
        "                                 title: str = 'Heat map with dendrogram',\n",
        "                                 figsize: Tuple[int, int] = (10, 10),\n",
        "                                 save_figure: bool = False,\n",
        "                                 file_name: str = 'results',\n",
        "                                 close_figure: bool = False) -> None:\n",
        "\n",
        "    # Validate input data\n",
        "    if not isinstance(clustered_dataframe, pd.DataFrame) or clustered_dataframe.ndim != 2:\n",
        "        raise ValueError(\"Input `clustered_dataframe` should be a 2D DataFrame.\")\n",
        "\n",
        "    if not is_valid_linkage(row_linkage_matrix):\n",
        "        raise ValueError(\"Input `row_linkage_matrix` is not a valid linkage matrix.\")\n",
        "\n",
        "    if not np.issubdtype(clustered_dataframe.values.dtype, np.number):\n",
        "        raise ValueError(\"Heatmap can only be plotted with numerical data.\")\n",
        "\n",
        "    # Create a figure to contain the plot elements\n",
        "    figure = plt.figure(figsize=figsize)\n",
        "\n",
        "    # Create a gridspec to handle the layout\n",
        "    grid_spec = figure.add_gridspec(2, 2, width_ratios=[0.05, 1], height_ratios=[0.2, 1], wspace=0.02, hspace=0.02)\n",
        "\n",
        "    # Add dendrogram on top\n",
        "    dendrogram_axis = figure.add_subplot(grid_spec[0, 1])\n",
        "    with plt.rc_context({'lines.linewidth': 0.5}):\n",
        "        dendro = dendrogram(row_linkage_matrix, ax=dendrogram_axis, orientation='top', color_threshold=color_threshold)\n",
        "    dendrogram_axis.axis('off')\n",
        "\n",
        "    # Create a color map\n",
        "    clusters = fcluster(row_linkage_matrix, color_threshold, criterion='distance')\n",
        "    unique_clusters = len(np.unique(clusters))\n",
        "\n",
        "    colors = sns.color_palette('tab20', n_colors=unique_clusters)\n",
        "    color_map = dict(enumerate(colors, 1))\n",
        "\n",
        "    # Change the color of each line to match the cluster colors\n",
        "    for i, d, c in zip(dendro['icoord'], dendro['dcoord'], clusters):\n",
        "        for j in range(4):\n",
        "            x = 0.5 * sum(i[j:j+2])\n",
        "            y = d[j]\n",
        "            dendrogram_axis.plot(x, y, color=color_map[c])\n",
        "\n",
        "    # Add heatmap\n",
        "    heatmap_axis = figure.add_subplot(grid_spec[1, 1])\n",
        "    sns.heatmap(clustered_dataframe, annot=False, ax=heatmap_axis, cbar=False, xticklabels=False, yticklabels=False)\n",
        "\n",
        "    # Add title to the entire figure\n",
        "    figure.suptitle(title, fontsize=10, y=0.91)\n",
        "\n",
        "    plt.tick_params(labelsize=5)\n",
        "    if save_figure:\n",
        "        plt.savefig(f\"{file_name}_heatmap.png\")\n",
        "        plt.savefig(f\"{file_name}_heatmap.pdf\")\n",
        "    if close_figure:\n",
        "        plt.close()\n",
        "    plt.show()\n",
        "\n",
        "def optimal_dendrogram_threshold_and_clusters(linkage_matrix: np.ndarray, N: int) -> float:\n",
        "    \"\"\"\n",
        "    Find the threshold for the linkage matrix such that the number of clusters is\n",
        "    approximately N. If a suitable threshold cannot be found, return a default value.\n",
        "    \"\"\"\n",
        "\n",
        "    # Starting threshold\n",
        "    threshold = linkage_matrix[-N, 2] if N < linkage_matrix.shape[0] else linkage_matrix[0, 2]\n",
        "\n",
        "    # Small value to adjust the threshold in each step\n",
        "    delta = 0.001\n",
        "\n",
        "    # Default threshold as the average of all distances\n",
        "    default_threshold = np.mean(linkage_matrix[:, 2])\n",
        "\n",
        "    while True:\n",
        "        clusters = fcluster(linkage_matrix, threshold, criterion='distance')\n",
        "        unique_clusters = len(np.unique(clusters))\n",
        "\n",
        "        if unique_clusters == N:\n",
        "            return threshold\n",
        "\n",
        "        if unique_clusters < N:\n",
        "            threshold -= delta\n",
        "        else:\n",
        "            threshold += delta\n",
        "\n",
        "        # Return default threshold if search goes out of bounds\n",
        "        if threshold < 0 or threshold > linkage_matrix[-1, 2]:\n",
        "            print(\"Warning: Couldn't find a suitable threshold for the specified number of clusters.\")\n",
        "            print(f\"Using a default threshold of {default_threshold}\")\n",
        "            return default_threshold\n",
        "\n",
        "def plot_time_length_regression(\n",
        "    virus_name: str,\n",
        "    meta_df: pd.DataFrame,\n",
        "    intervals_column: str,\n",
        "    color_threshold: float = None,\n",
        "    clustered_dataframe: pd.DataFrame = None,\n",
        "    row_linkage_matrix: np.ndarray = None,\n",
        "    grouping_column: Optional[str] = None,\n",
        "    title: str = 'Time and ZNA length regression',\n",
        "    legend: str = 'Cluster: ',\n",
        "    remove_outliers: Tuple[bool, int] = (False, 3),\n",
        "    figsize: Tuple[int, int] = (10, 5),\n",
        "    point_size: int = 15,\n",
        "    save_figure: bool = False,\n",
        "    normalize: bool = False,\n",
        "    file_name: str = 'results',\n",
        "    legend_loc: str = 'best',\n",
        "    close_figure: bool = False) -> None:\n",
        "    \"\"\"\n",
        "    Plot a time-length regression of Z-RNA regions' length against the collection date for different lineages.\n",
        "\n",
        "    Parameters:\n",
        "    - virus_name (str): Name of the virus to filter data.\n",
        "    - meta_df (pd.DataFrame): DataFrame containing the data to be plotted.\n",
        "    - intervals_column (str): The column name for length intervals in `meta_df`.\n",
        "    - clustered_dataframe (pd.DataFrame): DataFrame with clustered data.\n",
        "    - row_linkage_matrix (np.ndarray): Linkage matrix for hierarchical clustering.\n",
        "    - color_threshold (float): Threshold to use for coloring clusters.\n",
        "    - grouping_column (Optional[str]): The column for grouping data; defaults to None.\n",
        "    - title (str): Title of the plot; defaults to 'Time and ZNA length regression'.\n",
        "    - legend (str): Base string for the legend of each cluster; defaults to 'Cluster: '.\n",
        "    - remove_outliers (Tuple[bool, int]): Indicator and z-score threshold for outlier removal; defaults to (False, 3).\n",
        "    - figsize (Tuple[int, int]): Figure size; defaults to (10, 5).\n",
        "    - point_size (int): The size of points in the scatter plot; defaults to 8.\n",
        "    - save_figure (bool): Whether to save the figure as a PNG; defaults to False.\n",
        "    - normalize (bool): Whether to normalize the data; defaults to False.\n",
        "    - file_name (str): The name of the file to save the figure; defaults to 'results'.\n",
        "    - legend_loc (str): The location of the legend; defaults to 'best'.\n",
        "    - close_figure (bool): Whether to close the figure after plotting; defaults to False.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "\n",
        "    This function first processes the input data to ensure it's ready for plotting.\n",
        "    It then uses this data to plot the time-length regression and optionally a regression line.\n",
        "    The function can handle and visualize different lineage clusters,\n",
        "    providing a comprehensive view of the data spread across time and length.\n",
        "    Additional features include outlier removal, data normalization, and customized figure saving.\n",
        "    \"\"\"\n",
        "\n",
        "    # Make an explicit copy\n",
        "    df_copy = meta_df[meta_df['virus_species_from_file'] == virus_name].copy()\n",
        "\n",
        "    # Convert 'date' to datetime format only once\n",
        "    df_copy['date'] = pd.to_datetime(df_copy['date'], errors='coerce')\n",
        "\n",
        "    # Normalize the data if specified\n",
        "    if normalize:\n",
        "        df_copy['Normalized_Length'] = df_copy[intervals_column] / df_copy['Sequence Length']\n",
        "        length_column = 'Normalized_Length'\n",
        "    else:\n",
        "        length_column = intervals_column\n",
        "\n",
        "    # Convert the collection date to datetime and ordinal\n",
        "    df_copy['date'] = pd.to_datetime(df_copy['date'], errors='coerce')\n",
        "    df_copy = df_copy.dropna(subset=['date'])\n",
        "    df_copy['Date_Ordinal'] = df_copy['date'].apply(lambda x: x.toordinal())\n",
        "\n",
        "    # Remove outliers if specified\n",
        "    if remove_outliers[0]:\n",
        "        z_scores = df_copy[['Date_Ordinal', intervals_column]].apply(\n",
        "            lambda x: (x - x.mean()) / x.std()\n",
        "        )\n",
        "        df_copy = df_copy[\n",
        "            (np.abs(z_scores['Date_Ordinal']) <= remove_outliers[1]) &\n",
        "            (np.abs(z_scores[intervals_column]) <= remove_outliers[1])\n",
        "        ]\n",
        "\n",
        "    # Fit a linear regression model\n",
        "    X = df_copy[['Date_Ordinal']]\n",
        "    y = df_copy[length_column]\n",
        "    regression_model = LinearRegression()\n",
        "    regression_model.fit(X, y)\n",
        "    y_pred = regression_model.predict(X)\n",
        "\n",
        "    # Create the figure\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    if grouping_column != None:\n",
        "        unique_lineages = df_copy[grouping_column].unique()\n",
        "        colors = mpl.rcParams['axes.prop_cycle'].by_key()['color'][1:len(unique_lineages)+2]\n",
        "        color_map = dict(zip(sorted(unique_lineages), colors))\n",
        "        # Plot the data points for each cluster\n",
        "        for linage in set(df_copy[grouping_column]):\n",
        "            plt.scatter(df_copy[df_copy[grouping_column] == linage]['date'],\n",
        "                        df_copy[df_copy[grouping_column] == linage][length_column],\n",
        "                        color=color_map[linage], label=f\"{linage}\", s=point_size)\n",
        "    else:\n",
        "        grouping_column = 'leaves_color'\n",
        "        dendro = dendrogram(row_linkage_matrix, color_threshold=color_threshold, no_plot=True)\n",
        "        d = dict(zip(clustered_dataframe.columns, dendro['leaves_color_list']))\n",
        "        df_copy['leaves_color'] = df_copy['accession_with_version'].apply(lambda x: d.get(x))\n",
        "\n",
        "        unique_lineages = df_copy[grouping_column].unique()\n",
        "        colors = mpl.rcParams['axes.prop_cycle'].by_key()['color'][1:len(unique_lineages)+2]\n",
        "        color_map = dict(zip(sorted(unique_lineages), colors))\n",
        "        # Plot the data points for each cluster\n",
        "        for linage in set(df_copy[grouping_column]):\n",
        "            plt.scatter(df_copy[df_copy[grouping_column] == linage]['date'],\n",
        "                        df_copy[df_copy[grouping_column] == linage][length_column],\n",
        "                        color=color_map[linage], label=f\"{linage}\", s=point_size)\n",
        "\n",
        "    # Plot the regression line\n",
        "    plt.plot(df_copy['date'], y_pred, color='red', label='Regression Line')\n",
        "\n",
        "    # Get the slope and intercept of the regression line\n",
        "    slope = regression_model.coef_[0]\n",
        "    intercept = regression_model.intercept_\n",
        "\n",
        "    # Create a list of legend elements\n",
        "    legend_elements = [\n",
        "        Line2D([0], [0], marker='o', color='w', label=f\"{legend} {linage}\",\n",
        "               markerfacecolor=color, markersize=10)\n",
        "        for linage, color in color_map.items()\n",
        "    ]\n",
        "    legend_elements.append(Line2D([0], [0], color='red', lw=2, label='Regression Line'))\n",
        "    legend_elements.append(Line2D([0], [0], marker='None', color='w',\n",
        "                                  label=f\"Slope = {slope:.2f}\\nIntercept = {intercept:.2f}\"))\n",
        "\n",
        "    # Customize the plot\n",
        "    plt.legend(handles=legend_elements, loc=legend_loc, framealpha=0.5)\n",
        "    plt.grid(visible=True, which='major', axis='both', linestyle='-')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Collection Date')\n",
        "    plt.ylabel('Z-RNA regions total length')\n",
        "\n",
        "    # Save the figure if specified\n",
        "    if save_figure:\n",
        "        plt.savefig(f\"{file_name}_heatmap.png\")\n",
        "        plt.savefig(f\"{file_name}_heatmap.pdf\")\n",
        "\n",
        "    # Close the figure if specified\n",
        "    if close_figure:\n",
        "        plt.close()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "def remove_outliers(df: pd.DataFrame, column_names: list, multiplier: float = 3) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Removes outliers from a DataFrame based on values in specified columns using the IQR method.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The DataFrame to process.\n",
        "        column_names (list): A list of column names in which to check for outliers.\n",
        "        multiplier (float): The multiplier for the IQR. Defaults to 1.5.\n",
        "        Increase this value to be more lenient with outliers, and decrease it to be stricter.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with outliers removed.\n",
        "    \"\"\"\n",
        "\n",
        "    for column_name in column_names:\n",
        "        # Calculate Q1, Q2, and IQR\n",
        "        Q1 = df[column_name].quantile(0.25)\n",
        "        Q3 = df[column_name].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "\n",
        "        # Define bounds\n",
        "        lower_bound = Q1 - multiplier * IQR\n",
        "        upper_bound = Q3 + multiplier * IQR\n",
        "\n",
        "        # Filter the data frame\n",
        "        df = df[(df[column_name] >= lower_bound) & (df[column_name] <= upper_bound)]\n",
        "\n",
        "    return df\n",
        "\n",
        "def draw_boxplot_species(\n",
        "        data_subset: pd.DataFrame,\n",
        "        category_label: str = 'Virus name',\n",
        "        value_label: str = 'ZDNABERT intervals total length',\n",
        "        plot_title: str = '',\n",
        "        figure_size: Tuple[int, int] = (15, 6),\n",
        "        text_position: Tuple[float, float] = (0.02, 0.02),\n",
        "        legend_location: str = 'best') -> None:\n",
        "    \"\"\"\n",
        "    Draws a boxplot for certain categories.\n",
        "\n",
        "    Parameters:\n",
        "    data_subset (pd.DataFrame): The subset of the data to be plotted.\n",
        "    category_label (str): The name of the column representing the category.\n",
        "    value_label (str): The name of the column representing the values.\n",
        "    plot_title (str): The title of the plot.\n",
        "    figure_size (Tuple[int, int]): The size of the figure.\n",
        "    text_position (Tuple[float, float]): The position of the text inside the plot.\n",
        "    legend_location (str): The location of the legend.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # The unique categories\n",
        "    categories = list(data_subset[category_label].unique())\n",
        "\n",
        "    # Mapping English letters to Greek letters\n",
        "    greek_dict = {\n",
        "        'alphacoronavirus': r'$\\alpha$',\n",
        "        'betacoronavirus': r'$\\beta$',\n",
        "        'gammacoronavirus': r'$\\gamma$',\n",
        "        'deltacoronavirus': r'$\\delta$',\n",
        "        'alphapironavirus': r'$\\pi$',\n",
        "    }\n",
        "\n",
        "    # Specify the species groups\n",
        "    species_groups = ['Bats', 'Birds', 'Cats', 'Chicken', 'Hedgehog', 'Human',\n",
        "                      'Human/Swine/Dog', 'Mice', 'Mink', 'Myodes', 'Rat', 'Salmon',\n",
        "                      'Shrew', 'Swine', 'Whale']\n",
        "\n",
        "    # Filter the DataFrame to only include rows where 'species' is in species_groups\n",
        "    data_subset = data_subset[data_subset['Host'].isin(species_groups)]\n",
        "\n",
        "    # Sort dataframe by species\n",
        "    data_subset = data_subset.sort_values(by=['Host', 'virus_species_from_file'])\n",
        "\n",
        "    # Store the 'Intervals total length' for each category\n",
        "    category_values = []\n",
        "    for category in categories:\n",
        "        values = data_subset[data_subset[category_label] == category][value_label].values\n",
        "        if not np.isnan(values).any():\n",
        "            category_values.append(values)\n",
        "\n",
        "    # Assuming that this function exists\n",
        "    category_values = filter_arrays(category_values)\n",
        "\n",
        "    # Perform the ANOVA\n",
        "    f_value, p_value = stats.f_oneway(*category_values)\n",
        "\n",
        "    # Create a figure and axis for the plot\n",
        "    fig, ax = plt.subplots(figsize=figure_size)\n",
        "\n",
        "    # Create the boxplot with seaborn\n",
        "    box_plot = sns.boxplot(\n",
        "        x=category_label,\n",
        "        y=value_label,\n",
        "        hue='Host',\n",
        "        data=data_subset,\n",
        "        dodge=False\n",
        "    )\n",
        "\n",
        "    # Add a title and labels\n",
        "    ax.set_title(plot_title)\n",
        "    ax.set_xlabel('Viruses')\n",
        "    ax.set_ylabel(f'Z-RNA {value_label}')\n",
        "\n",
        "    # Get color palette\n",
        "    palette = sns.color_palette(\"husl\", 20)\n",
        "\n",
        "    # Create a mapping of labels to species\n",
        "    label_to_species = data_subset.groupby(category_label)['Host'].agg(pd.Series.mode).to_dict()\n",
        "\n",
        "    # Create a mapping of species to colors\n",
        "    species_to_colors = dict(zip(species_groups, palette))\n",
        "\n",
        "    # Mapping taxa to colors\n",
        "    taxa_to_colors = {\n",
        "        'alphapironavirus': '#FF6B6B',  # Light Red\n",
        "        'alphacoronavirus': '#4ECDC4',  # Turquoise\n",
        "        'betacoronavirus': '#556270',   # Dark Grayish Blue\n",
        "        'deltacoronavirus': '#C7F464',  # Light Yellow Green\n",
        "        'gammacoronavirus': '#FFA577'   # Light Orange\n",
        "    }\n",
        "\n",
        "    # Apply the colors to the labels based on the most common species for that label\n",
        "    for i, label_text in enumerate(ax.get_xticklabels()):\n",
        "        species = label_to_species[label_text.get_text()]\n",
        "        color = species_to_colors[species]\n",
        "        label_text.set_color(color)\n",
        "\n",
        "        # Extract the corresponding 'taxa' value for the label\n",
        "        taxa = data_subset[data_subset[category_label] == label_text.get_text()]['genus'].unique()[0]\n",
        "        # Convert first letter to Greek equivalent\n",
        "        taxa = taxa.lower()\n",
        "        taxa_greek = greek_dict.get(taxa, taxa[0])\n",
        "\n",
        "        # Update y position to below y=0 and set color based on taxa\n",
        "        ax.text(\n",
        "            (i + 0.5) / len(categories), 0.01, taxa_greek,\n",
        "            horizontalalignment='center', size='small', color='black',\n",
        "            weight='semibold', transform=ax.transAxes,\n",
        "            bbox=dict(facecolor=taxa_to_colors[taxa], alpha=0.4, boxstyle='round,pad=0.2')\n",
        "        )\n",
        "\n",
        "    ax.set_xticklabels(ax.get_xticklabels(), rotation=90, fontsize=10, fontweight='bold')\n",
        "\n",
        "    # Set the legend title\n",
        "    ax.legend(title='Host species', loc=legend_location)\n",
        "\n",
        "    # Add the F-value, p-value, and test name inside the plot\n",
        "    anova_text = f'ANOVA Test\\nF-value: {f_value:.2f}\\nP-value: {p_value:.4f}'\n",
        "    ax.text(\n",
        "        *text_position, anova_text, transform=ax.transAxes, fontsize=10,\n",
        "        verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "def pango_to_who(pango_lineage: str) -> str:\n",
        "    \"\"\"\n",
        "    Converts Pango lineage of SARS-CoV-2 virus to WHO label.\n",
        "\n",
        "    Args:\n",
        "        pango_lineage (str): A string representing the Pango lineage of the virus.\n",
        "\n",
        "    Returns:\n",
        "        str: A string representing the WHO label.\n",
        "    \"\"\"\n",
        "    pango_lineage = pango_lineage.upper()\n",
        "\n",
        "    pango_to_who_map = {\n",
        "        \"B.1.1.7\": \"Alpha\",\n",
        "        \"Q\": \"Alpha\",\n",
        "        \"B.1.351\": \"Beta\",\n",
        "        \"P.1\": \"Gamma\",\n",
        "        \"B.1.617.2\": \"Delta\",\n",
        "        \"B.1.617.1\": \"Kappa\",\n",
        "        \"B.1.617.3\": \"Kappa\",\n",
        "        \"B.1.427\": \"Epsilon\",\n",
        "        \"B.1.429\": \"Epsilon\",\n",
        "        \"B.1.525\": \"Eta\",\n",
        "        \"B.1.526\": \"Iota\",\n",
        "        \"C.37\": \"Lambda\",\n",
        "        \"B.1.621\": \"Mu\",\n",
        "        \"B.1.621.1\": \"Mu\",\n",
        "        \"BA.1\": \"Omicron\",\n",
        "        \"BA.2\": \"Omicron\",\n",
        "        \"BA.4\": \"Omicron\",\n",
        "        \"BA.5\": \"Omicron\",\n",
        "        \"BA.2.12.1\": \"Omicron\",\n",
        "        \"BA.2.75\": \"Omicron\",\n",
        "        \"BQ.1\": \"Omicron\",\n",
        "        \"XBB.1.5\": \"Omicron\",\n",
        "        \"XBB.1.16\": \"Omicron\",\n",
        "        \"P.2\": \"Zeta\"\n",
        "    }\n",
        "\n",
        "    for pango, who in pango_to_who_map.items():\n",
        "        if pango_lineage.startswith(pango):\n",
        "            return who\n",
        "\n",
        "    return \"Unknown\"\n",
        "\n",
        "def draw_boxplot_who(\n",
        "        df_subset: pd.DataFrame,\n",
        "        label: str = 'WHO',\n",
        "        values: str = 'Intervals Total Length',\n",
        "        title: str = '',\n",
        "        figsize: Tuple[int, int] = (15, 6),\n",
        "        pos: Tuple[float, float] = (0.02, 0.02)) -> None:\n",
        "    \"\"\"\n",
        "    Draws a boxplot for a given DataFrame.\n",
        "\n",
        "    Parameters:\n",
        "    df_subset (pd.DataFrame): The subset of the data to be plotted.\n",
        "    label (str): The name of the column representing the category.\n",
        "    values (str): The name of the column representing the values.\n",
        "    title (str): The title of the plot.\n",
        "    figsize (Tuple[int, int]): The size of the figure.\n",
        "    pos (Tuple[float, float]): The position of the text inside the plot.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "    # Pangolin values\n",
        "    pangolin_values = ['Alpha', 'Beta', 'Gamma', 'Delta', 'Epsilon',\n",
        "                       'Eta', 'Iota', 'Kappa', 'Lambda', 'Mu',\n",
        "                       'Omicron', 'Unknown', 'Zeta']\n",
        "\n",
        "    # Create an empty list to store the 'Intervals total length' for each 'Pangolin' value\n",
        "    data = []\n",
        "\n",
        "    # For each 'Pangolin' value, append the 'Intervals total length' to the data list\n",
        "    for pangolin in pangolin_values:\n",
        "        data.append(df_subset[df_subset[label] == pangolin][values].values)\n",
        "\n",
        "    # Assuming that this function exists\n",
        "    data = filter_arrays(data)\n",
        "\n",
        "    # Perform the ANOVA\n",
        "    f_value, p_value = stats.f_oneway(*data)\n",
        "\n",
        "    # Create a figure and axis for the plot\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Create the boxplot with seaborn\n",
        "    sns.boxplot(x=label, y=values, data=df_subset, order=pangolin_values, ax=ax)\n",
        "\n",
        "    # Add a title and labels\n",
        "    ax.set_title(title)\n",
        "    ax.set_xlabel(label)\n",
        "    ax.set_ylabel(f'Z-RNA {values}')\n",
        "\n",
        "    # Add the F-value, p-value, and test name inside the plot\n",
        "    anova_text = f'ANOVA Test\\nF-value: {f_value:.2f}\\nP-value: {p_value:.2f}'\n",
        "    ax.text(\n",
        "        *pos, anova_text, transform=ax.transAxes, fontsize=10,\n",
        "        verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
        "    )\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "def plot_time_length_linages_boxplot(\n",
        "    meta_df: pd.DataFrame,\n",
        "    length_column: str = '',\n",
        "    title: str = '',\n",
        "    remove_outliers: Tuple[bool, int] = (False, 3),\n",
        "    figsize: Tuple[int, int] = (14, 8),\n",
        "    text_position: Tuple[float, float] = (0.01, 0.90),\n",
        "    save_figure: bool = False,\n",
        "    normalize: bool = False,\n",
        "    file_name: str = 'results',\n",
        "    legend_loc: str = 'best',\n",
        "    close_figure: bool = False\n",
        ") -> None:\n",
        "\n",
        "    # Copy the dataframe\n",
        "    df_copy = meta_df.copy()\n",
        "\n",
        "    # Normalize the data if specified\n",
        "    if normalize:\n",
        "        df_copy['Normalized_Length'] = df_copy['Intervals Total Length'] / df_copy['Sequence Length']\n",
        "        length_column = 'Normalized_Length'\n",
        "\n",
        "    # Convert the collection date to datetime and extract year and month\n",
        "    df_copy['datetime'] = pd.to_datetime(df_copy['datetime'], errors='coerce')\n",
        "    df_copy['Year_Month'] = df_copy['datetime'].dt.to_period('M')\n",
        "\n",
        "    # Order by year and month\n",
        "    df_copy['Year_Month'] = df_copy['Year_Month'].astype(\n",
        "        CategoricalDtype(\n",
        "            categories=sorted(df_copy['Year_Month'].unique()),\n",
        "            ordered=True\n",
        "        )\n",
        "    )\n",
        "\n",
        "    # Remove outliers if specified\n",
        "    if remove_outliers[0]:\n",
        "        z_scores = df_copy[[length_column]].apply(lambda x: (x - x.mean()) / x.std())\n",
        "        df_copy = df_copy[(np.abs(z_scores[length_column]) <= remove_outliers[1])]\n",
        "\n",
        "    # Create the figure\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Plot the boxplot for each month using seaborn\n",
        "    sns.boxplot(x=\"Year_Month\", y=length_column, data=df_copy, palette='viridis', width=0.5, ax=ax)\n",
        "\n",
        "    # Perform ANOVA\n",
        "    groups = [group[length_column].dropna() for name, group in df_copy.groupby('Year_Month')]\n",
        "    f_value, p_value = stats.f_oneway(*groups)\n",
        "\n",
        "    # Add the F-value, p-value, and test name inside the plot\n",
        "    anova_text = f'ANOVA Test\\nF-value: {f_value:.2f}\\nP-value: {p_value:.4f}'\n",
        "    ax.text(\n",
        "        *text_position, anova_text, transform=ax.transAxes, fontsize=10,\n",
        "        verticalalignment='bottom', bbox=dict(boxstyle='round', facecolor='white', alpha=0.5)\n",
        "    )\n",
        "\n",
        "    # Customize the plot\n",
        "    ax.set_title(title, fontsize=16)\n",
        "    ax.set_xlabel('Collection Date', fontsize=14)\n",
        "    ax.set_ylabel('Z-RNA Regions Length', fontsize=14)\n",
        "    plt.xticks(rotation=90, fontsize=10)\n",
        "\n",
        "    # Save the figure if specified\n",
        "    if save_figure:\n",
        "        plt.savefig(f\"{file_name}_boxplot.png\", dpi=300)\n",
        "        plt.savefig(f\"{file_name}_boxplot.pdf\", dpi=300)\n",
        "\n",
        "    # Close the figure if specified\n",
        "    if close_figure:\n",
        "        plt.close()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()\n",
        "\n",
        "def plot_time_length_regression_linages(\n",
        "    meta_df: pd.DataFrame,\n",
        "    length_column: str = '',\n",
        "    category_column: str = 'WHO',\n",
        "    title: str = '',\n",
        "    remove_outliers: Tuple[bool, int] = (False, 3),\n",
        "    figsize: Tuple[int, int] = (10, 5),\n",
        "    point_size: int = 3,\n",
        "    save_figure: bool = False,\n",
        "    normalize: bool = False,\n",
        "    file_name: str = 'results',\n",
        "    legend_loc: str = 'best',\n",
        "    close_figure: bool = False,\n",
        "    show_legend: bool = True\n",
        ") -> None:\n",
        "    \"\"\"\n",
        "    Plot a time-length regression of Z-RNA regions' length against the collection date for different lineages.\n",
        "\n",
        "    Parameters:\n",
        "    meta_df (pd.DataFrame): The dataframe containing the data to be plotted.\n",
        "    title (str, optional): The title of the plot. Defaults to 'Time and ZNA length regression'.\n",
        "    remove_outliers (Tuple[bool, int], optional): A tuple indicating whether to remove outliers\n",
        "    and the z-score threshold for outlier removal. Defaults to (False, 3).\n",
        "    figsize (Tuple[int, int], optional): The size of the figure. Defaults to (10, 5).\n",
        "    point_size (int, optional): The size of the points in the scatter plot. Defaults to 3.\n",
        "    save_figure (bool, optional): Whether to save the figure as a PNG. Defaults to False.\n",
        "    normalize (bool, optional): Whether to normalize the data. Defaults to False.\n",
        "    file_name (str, optional): The name of the file to save the figure as. Defaults to 'results'.\n",
        "    legend_loc (str, optional): The location of the legend. Defaults to 'best'.\n",
        "    close_figure (bool, optional): Whether to close the figure after plotting. Defaults to False.\n",
        "\n",
        "    Returns:\n",
        "    None\n",
        "    \"\"\"\n",
        "\n",
        "    # Copy the dataframe\n",
        "    df_copy = meta_df.copy()\n",
        "\n",
        "    # Normalize the data if specified\n",
        "    if normalize:\n",
        "        df_copy['Normalized_Length'] = df_copy['Intervals Total Length'] / df_copy['Sequence Length']\n",
        "        length_column = 'Normalized_Length'\n",
        "\n",
        "    # Convert the collection date to datetime and ordinal\n",
        "    df_copy['Collection_Date'] = pd.to_datetime(df_copy['datetime'], errors='coerce')\n",
        "    df_copy = df_copy.dropna(subset=['Collection_Date'])\n",
        "    df_copy['Date_Ordinal'] = df_copy['Collection_Date'].apply(lambda x: x.toordinal())\n",
        "\n",
        "    # Remove outliers if specified\n",
        "    if remove_outliers[0]:\n",
        "        z_scores = df_copy[['Date_Ordinal', 'Intervals Total Length']].apply(\n",
        "            lambda x: (x - x.mean()) / x.std()\n",
        "        )\n",
        "        df_copy = df_copy[\n",
        "            (np.abs(z_scores['Date_Ordinal']) <= remove_outliers[1]) &\n",
        "            (np.abs(z_scores['Intervals Total Length']) <= remove_outliers[1])\n",
        "        ]\n",
        "\n",
        "    # Create a color map\n",
        "    unique_values = df_copy[category_column].unique()\n",
        "    colors = get_high_contrast_colors_gr(len(unique_values))\n",
        "    color_map = dict(zip(unique_values, colors))\n",
        "\n",
        "    # Fit a linear regression model\n",
        "    X = df_copy[['Date_Ordinal']]\n",
        "    y = df_copy[length_column]\n",
        "    regression_model = LinearRegression()\n",
        "    regression_model.fit(X, y)\n",
        "    y_pred = regression_model.predict(X)\n",
        "\n",
        "    # Create the figure\n",
        "    plt.figure(figsize=figsize)\n",
        "\n",
        "    # Plot the data points for each cluster\n",
        "    for linage in set(df_copy[category_column]):\n",
        "        plt.scatter(df_copy[df_copy[category_column] == linage]['Collection_Date'],\n",
        "                    df_copy[df_copy[category_column] == linage][length_column],\n",
        "                    color=color_map[linage], label=f\"{linage}\", s=point_size)\n",
        "\n",
        "    # Plot the regression line\n",
        "    plt.plot(df_copy['Collection_Date'], y_pred, color='red', label='Regression Line')\n",
        "\n",
        "    # Get the slope and intercept of the regression line\n",
        "    slope = regression_model.coef_[0]\n",
        "    intercept = regression_model.intercept_\n",
        "\n",
        "    # Create a list of legend elements\n",
        "    if show_legend:\n",
        "        legend_elements = [\n",
        "            Line2D([0], [0], marker='o', color='w', label=f\"{linage}\",\n",
        "                   markerfacecolor=color, markersize=10)\n",
        "            for linage, color in color_map.items()\n",
        "        ]\n",
        "        legend_elements.append(Line2D([0], [0], color='red', lw=2, label='Regression Line'))\n",
        "        legend_elements.append(Line2D([0], [0], marker='None', color='w',\n",
        "                                      label=f\"Slope = {slope:.2f}\\nIntercept = {intercept:.2f}\"))\n",
        "        plt.legend(handles=legend_elements, loc=legend_loc, framealpha=0.5)\n",
        "\n",
        "    # Customize the plot\n",
        "    plt.grid(visible=True, which='major', axis='both', linestyle='-')\n",
        "    plt.title(title)\n",
        "    plt.xlabel('Collection Date')\n",
        "    plt.ylabel('Z-RNA Regions Length')\n",
        "\n",
        "    # Save the figure if specified\n",
        "    if save_figure:\n",
        "        plt.savefig(f\"{file_name}_heatmap.png\")\n",
        "        plt.savefig(f\"{file_name}_heatmap.pdf\")\n",
        "\n",
        "    # Close the figure if specified\n",
        "    if close_figure:\n",
        "        plt.close()\n",
        "\n",
        "    # Show the plot\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8sGOZCg28nN"
      },
      "source": [
        "# Филогенетический анализ референсных штаммов"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kFGRQu9oQaK9"
      },
      "outputs": [],
      "source": [
        "taxa_db.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUDgqX3o3QL0"
      },
      "outputs": [],
      "source": [
        "# Dowload refseq gb files from genbank\n",
        "%cd /content\n",
        "!mkdir refseqs\n",
        "\n",
        "accession_list = taxa_db['Accession']\n",
        "fetch_gb_files(accession_list, email=Entrez.email, output_dir=\"/content/refseqs\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rHXYjIs83Rag"
      },
      "outputs": [],
      "source": [
        "directory = '/content/refseqs'\n",
        "file_paths = [os.path.join(directory, file)\n",
        "              for file in os.listdir(directory)\n",
        "              if os.path.isfile(os.path.join(directory, file))]\n",
        "\n",
        "protein_names_list = []\n",
        "\n",
        "for gb_file in file_paths:\n",
        "  protein_names_list.extend(list_proteins(gb_file))\n",
        "\n",
        "unique_protein_names = sorted(set(protein_names_list))\n",
        "print(unique_protein_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5QHsMlqO3RUE"
      },
      "outputs": [],
      "source": [
        "# All nucleocapsid protein aliases used in gb annotations\n",
        "protein_names = ['nucleocapsid',\n",
        "'nucleocapsid phosphoprotein',\n",
        "'nucleocapsid protein',\n",
        "'nucleoprotein',\n",
        "'N',\n",
        "'N protein',\n",
        "'capsid']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EN6AdHvd3RQ5"
      },
      "outputs": [],
      "source": [
        "directory = '/content/refseqs'\n",
        "file_paths = [os.path.join(directory, file)\n",
        "              for file in os.listdir(directory)\n",
        "              if os.path.isfile(os.path.join(directory, file))]\n",
        "seq_names = []\n",
        "sequences = []\n",
        "for gb_file in file_paths:\n",
        "  for pr in protein_names:\n",
        "    seq = extract_protein_sequence(gb_file, pr)\n",
        "    if seq:\n",
        "      seq_names.append(gb_file.split('/')[3][:-3])\n",
        "      sequences.append(seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2uGng-UZ3RIw"
      },
      "outputs": [],
      "source": [
        "# Two species are missing, no protein annotations\n",
        "missing = [i.split('/')[3][:-3] for i in file_paths if i.split('/')[3][:-3] not in seq_names]\n",
        "print(missing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pGpr9sNBlVc"
      },
      "outputs": [],
      "source": [
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T2vuiTt6Rdw"
      },
      "outputs": [],
      "source": [
        "# Combine nucleocapsid protein sequences inta a single fasta file\n",
        "write_to_fasta(seq_names, sequences, 'ref_sequences_nucleocapsid.fas')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8y5T0cb6T2Q"
      },
      "outputs": [],
      "source": [
        "# Multiple sequence alignment\n",
        "!mafft --auto /content/ref_sequences_nucleocapsid.fas > /content/ref_sequences_nucleocapsid_aln.fas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SG4wM5rE6Tzb"
      },
      "outputs": [],
      "source": [
        "# Alignment trimming\n",
        "!trimal/source/trimal -in /content/ref_sequences_nucleocapsid_aln.fas \\\n",
        "                      -out /content/ref_sequences_nucleocapsid_aln_trimm.fas \\\n",
        "                      -automated1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYEJnkqR6Twg"
      },
      "outputs": [],
      "source": [
        "# Substitution model estimation and tree building\n",
        "%cd /content/IQ-TREE/build\n",
        "!./iqtree -nt AUTO -s /content/ref_sequences_nucleocapsid_aln_trimm.fas -m TEST\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l65vm4ZcO7_4"
      },
      "source": [
        "# Анализ участков Z-РНК референсных штаммов семейства Coronaviridae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pz7QlhGET_Cq"
      },
      "outputs": [],
      "source": [
        "# Convert and combine all reference sequences' GenBank files into one FASTA file\n",
        "directory = '/content/refseqs'\n",
        "file_paths = [os.path.join(directory, file)\n",
        "              for file in os.listdir(directory)\n",
        "              if os.path.isfile(os.path.join(directory, file))]\n",
        "with open('refseqs_all.fas', 'w') as fasta_out:\n",
        "    for gb_file in file_paths:\n",
        "        for record in SeqIO.parse(gb_file, \"genbank\"):\n",
        "            SeqIO.write(record, fasta_out, \"fasta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bJzevYXrDrwp"
      },
      "outputs": [],
      "source": [
        "# Predict Z-RNA regions for reference sequences with ZDNABERT\n",
        "zdnabert_refseq_intervals = process_fasta_with_function('/content/refseqs_all.fas', run_zdnabert, ZDNABERT_PARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C8c6U0MZDkA"
      },
      "outputs": [],
      "source": [
        "# Predict Z-RNA regions for reference sequences with ZDNABERT\n",
        "zhunt_refseq_intervals = process_fasta_with_function('/content/refseqs_all.fas', run_zhunt, ZHUNT_PARAMS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OR2pb4-IDrtG"
      },
      "outputs": [],
      "source": [
        "# Saving results into a JSON file\n",
        "with open('refs_zdnabert.json', 'w') as fp:\n",
        "    json.dump(zdnabert_refseq_intervals, fp, cls=NumpyEncoder)\n",
        "\n",
        "with open('refs_zhunt.json', 'w') as fp:\n",
        "    json.dump(zhunt_refseq_intervals, fp, cls=NumpyEncoder)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qp2uCizagdXk"
      },
      "source": [
        "# Анализ участков Z-РНК для каждого вида вируса из семейства *Coronaviridae*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WyW280KSVEMK"
      },
      "outputs": [],
      "source": [
        "# Iterate over unique species and count strains\n",
        "unique_species = taxa_db[\"Species\"].unique()\n",
        "\n",
        "data = []\n",
        "for species in unique_species:\n",
        "    taxid = get_taxid(species)\n",
        "    if taxid:\n",
        "        entry_count = count_genbank_entries(taxid)\n",
        "        print(f\"Found {entry_count} strains for species: {species}.\")\n",
        "    else:\n",
        "        print(f\"No taxid found for species: {species}. Setting virus strains count to 0.\")\n",
        "        entry_count = 0\n",
        "    data.append((species, entry_count))\n",
        "    # Respect NCBI's request to not send more than 3 requests per second\n",
        "    time.sleep(0.35)\n",
        "\n",
        "# Create a DataFrame with species and their respective virus counts\n",
        "df = pd.DataFrame(data, columns=[\"Species\", \"Virus Count\"])\n",
        "df.to_csv('Spicies_counts.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8_eE9BnVEJC"
      },
      "outputs": [],
      "source": [
        "# Filter out 'severe acute respiratory syndrome-related coronavirus' from the dataset and fetch others\n",
        "filtered_taxa_db = taxa_db[taxa_db[\"Species\"] != \"Severe acute respiratory syndrome-related coronavirus\"][\n",
        "    \"Species\"\n",
        "].unique()\n",
        "\n",
        "for species in filtered_taxa_db:\n",
        "    print(f\"Processing: {species}\")\n",
        "    taxid = get_taxid(species)\n",
        "    if taxid:\n",
        "        print(f\"Found taxid: {taxid} for species: {species}\")\n",
        "        fetch_genbank_taxid(taxid, filename=f\"/content/species_gb_files/{species.replace(' ', '_')}.gbk\", n=1000)\n",
        "    else:\n",
        "        print(f\"No taxid found for species: {species}\")\n",
        "        time.sleep(0.35)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Hpl3anyVEGX"
      },
      "outputs": [],
      "source": [
        "# Search for SARS-CoV sequences excluding SARS-CoV-2\n",
        "query = 'SARS[Organism] NOT SARS-CoV-2[Organism] AND biomol_genomic[prop] AND (\"25000\"[SLEN] : \"35000\"[SLEN]'\n",
        "handle = Entrez.esearch(db=\"nucleotide\", term=query, retmax=10000)  # Set retmax to the desired number of results\n",
        "record = Entrez.read(handle)\n",
        "id_list = record[\"IdList\"]\n",
        "\n",
        "# Fetch the sequences\n",
        "sequences = []\n",
        "for seq_id in tqdm(id_list):\n",
        "    fetch_handle = Entrez.efetch(db=\"nucleotide\", id=seq_id, rettype=\"gb\", retmode=\"text\")\n",
        "    sequence = SeqIO.read(fetch_handle, \"genbank\")\n",
        "    sequences.append(sequence)\n",
        "    fetch_handle.close()\n",
        "\n",
        "# Save sequences to a file\n",
        "SeqIO.write(sequences,\n",
        "            \"/content/species_gb_files/Severe_acute_respiratory_syndrome_related_coronavirus.gbk\",\n",
        "            \"genbank\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tm1KbOazepS"
      },
      "outputs": [],
      "source": [
        "# Search for Human coronavirus OC43\n",
        "query = 'Human coronavirus OC43[Organism] AND biomol_genomic[prop] AND (\"25000\"[SLEN] : \"35000\"[SLEN]'\n",
        "handle = Entrez.esearch(db=\"nucleotide\", term=query, retmax=10000)  # Set retmax to the desired number of results\n",
        "record = Entrez.read(handle)\n",
        "id_list = record[\"IdList\"]\n",
        "\n",
        "# Fetch the sequences\n",
        "sequences = []\n",
        "for seq_id in tqdm(id_list):\n",
        "    fetch_handle = Entrez.efetch(db=\"nucleotide\", id=seq_id, rettype=\"gb\", retmode=\"text\")\n",
        "    sequence = SeqIO.read(fetch_handle, \"genbank\")\n",
        "    sequences.append(sequence)\n",
        "    fetch_handle.close()\n",
        "\n",
        "# Save sequences to a file\n",
        "SeqIO.write(sequences,\n",
        "            \"/content/species_gb_files/Human_coronavirus_OC43.gbk\",\n",
        "            \"genbank\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KKn0joo3YfW"
      },
      "outputs": [],
      "source": [
        "!zip -r /content/species_gb_files.zip /content/species_gb_files\n",
        "files.download(\"/content/species_gb_files.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YVYgf0q0A1SA"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/species_fasta_files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s-hb_Ecc7P-G"
      },
      "outputs": [],
      "source": [
        "# Convert and combine GenBank files into FASTA files\n",
        "file_paths = list_files('/content/species_gb_files/')\n",
        "\n",
        "for gb_file in file_paths:\n",
        "    virus_name = gb_file.split('/')[3][:-4]\n",
        "    with open(f'/content/species_fasta_files/{virus_name}.fas', 'w') as fasta_out:\n",
        "        for record in SeqIO.parse(gb_file, \"genbank\"):\n",
        "            SeqIO.write(record, fasta_out, \"fasta\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u12PLfwTMd7t"
      },
      "outputs": [],
      "source": [
        "!mkdir '/content/Coronaviridae strains predictions json'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwfTntXLA59_"
      },
      "outputs": [],
      "source": [
        "# Analyze Z-RNA regions and save results\n",
        "file_paths = list_files('/content/species_fasta_files/')\n",
        "\n",
        "for file in file_paths:\n",
        "    virus_name = file.split('/')[3][:-4]\n",
        "    zdnabert_refseq_intervals = process_fasta_with_function(file, run_zdnabert, ZDNABERT_PARAMS)\n",
        "    with open(f\"/content/species_json_dnabert/zdnabert_{virus_name}.json\", 'w') as results_file:\n",
        "        json.dump(zdnabert_refseq_intervals, results_file, cls=NumpyEncoder)\n",
        "\n",
        "    zhunt_refseq_intervals = process_fasta_with_function(file, run_zhunt, ZHUNT_PARAMS)\n",
        "    with open(f\"/content/species_json_zhunt/zhunt_{virus_name}.json\", 'w') as results_file:\n",
        "        json.dump(zdnabert_refseq_intervals, results_file, cls=NumpyEncoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fh6kIq2XVEAe"
      },
      "outputs": [],
      "source": [
        "# Download json files\n",
        "!zip -r /content/species_json_dnaber.zip /content/species_json_dnabert\n",
        "files.download(\"/content/species_json_dnaber.zip\")\n",
        "\n",
        "!zip -r /content/species_json_zhunt.zip /content/species_json_zhunt\n",
        "files.download(\"/content/species_json_zhunt.zip\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u09GKBtwhzC7"
      },
      "source": [
        "# Визуализация филогенетического дерева референсных штаммов\n",
        "### с указанием суммарной длины участков Z-РНК для ветвей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q9irP9xjW4_W"
      },
      "outputs": [],
      "source": [
        "tree_newick_string = open('/content/ref_sequences_nucleocapsid_aln_trimm.fas.treefile', 'r').read().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzFnuz5vjyrR"
      },
      "outputs": [],
      "source": [
        "length_of_intervals_zdnabert = total_interval_length(zdnabert_refseq_intervals)\n",
        "length_of_intervals_zhunt = total_interval_length(zhunt_refseq_intervals)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jY7BXk7IW45m"
      },
      "outputs": [],
      "source": [
        "plot_tree_with_annotations(tree_newick_string, length_of_intervals_zdnabert, taxa_db, color_threshold=0);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "12pre4QzW42m"
      },
      "outputs": [],
      "source": [
        "plot_tree_with_annotations(tree_newick_string, length_of_intervals_zhunt, taxa_db, color_threshold=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NS8dZOLHZU1"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XVHUA5j9H3B-"
      },
      "source": [
        "# Подготовка метаданных для последовательностей и участков Z-РНК"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfbiPt86INqS"
      },
      "source": [
        "## Данные по кластеризации участков Z-РНК"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tv9ROyXX9gfg"
      },
      "outputs": [],
      "source": [
        "!mkdir /content/clustered_data_frames_dnabert/\n",
        "!mkdir /content/clustered_data_frames_zhunt/\n",
        "!mkdir /content/row_linkage_matrices_dnabert/\n",
        "!mkdir /content/row_linkage_matrices_zhunt/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7cBwHAB6bot"
      },
      "outputs": [],
      "source": [
        "file_paths_zdnabert = sorted(list_files('/content/Coronaviridae strains predictions json/species_json_dnabert'))\n",
        "file_paths_zhunt = sorted(list_files('/content/Coronaviridae strains predictions json/species_json_zhunt'))\n",
        "\n",
        "for i, j in list(zip(file_paths_zdnabert, file_paths_zhunt)):\n",
        "    virus_name = j.split('_', 3)[3][:-5]\n",
        "\n",
        "    dnabert_intervals = read_json_file(i)\n",
        "    zhunt_intervals = read_json_file(j)\n",
        "\n",
        "    dnabert_cls_df, dnabert_rowl = create_clustered_dataframe(dnabert_intervals)\n",
        "    zhunt_cls_df, zhunt_rowl = create_clustered_dataframe(zhunt_intervals)\n",
        "\n",
        "    dnabert_cls_df.to_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus_name}.csv')\n",
        "    zhunt_cls_df.to_csv(f'/content/clustered_data_frames_zhunt/zhunt_cls_df_{virus_name}.csv')\n",
        "\n",
        "    np.save(f\"/content/row_linkage_matrices_dnabert/dnabert_rowl_{virus_name}\", dnabert_rowl)\n",
        "    np.save(f\"/content/row_linkage_matrices_zhunt/zhunt_rowl_{virus_name}\", zhunt_rowl)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dnabert_intervals = read_json_file('/content/species_json_dnabert/zdnabert_Human_coronavirus_OC43.json')\n",
        "zhunt_intervals = read_json_file('/content/species_json_zhunt/zhunt_Human_coronavirus_OC43.json')\n",
        "\n",
        "dnabert_cls_df, dnabert_rowl = create_clustered_dataframe(dnabert_intervals)\n",
        "zhunt_cls_df, zhunt_rowl = create_clustered_dataframe(zhunt_intervals)\n",
        "\n",
        "dnabert_cls_df.to_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_Human_coronavirus_OC43.csv')\n",
        "zhunt_cls_df.to_csv(f'/content/clustered_data_frames_zhunt/zhunt_cls_df_Human_coronavirus_OC43.csv')\n",
        "\n",
        "np.save(f\"/content/row_linkage_matrices_dnabert/dnabert_rowl_Human_coronavirus_OC43\", dnabert_rowl)\n",
        "np.save(f\"/content/row_linkage_matrices_zhunt/zhunt_rowl_Human_coronavirus_OC43\", zhunt_rowl)"
      ],
      "metadata": {
        "id": "mv0kQLBLx2Qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "59GvYyvRsstJ"
      },
      "outputs": [],
      "source": [
        "# Download csv files\n",
        "!zip -r /content/clustered_data_frames_dnabert.zip /content/clustered_data_frames_dnabert\n",
        "files.download(\"/content/clustered_data_frames_dnabert.zip\")\n",
        "!zip -r /content/clustered_data_frames_zhunt.zip /content/clustered_data_frames_zhunt\n",
        "files.download(\"/content/clustered_data_frames_zhunt.zip\")\n",
        "\n",
        "!zip -r /content/row_linkage_matrices_dnabert.zip /content/row_linkage_matrices_dnabert\n",
        "files.download(\"/content/row_linkage_matrices_dnabert.zip\")\n",
        "!zip -r /content/row_linkage_matrices_zhunt.zip /content/row_linkage_matrices_zhunt\n",
        "files.download(\"/content/row_linkage_matrices_zhunt.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJZlkpU0ktjA"
      },
      "outputs": [],
      "source": [
        "viruses = [\n",
        "    'Severe_acute_respiratory_syndrome_related_coronavirus',\n",
        "    'Middle_East_respiratory_syndrome-related_coronavirus',\n",
        "    'Human_coronavirus_229E',\n",
        "    'Human_coronavirus_HKU1',\n",
        "    'Human_coronavirus_NL63',\n",
        "    'Human_coronavirus_OC43',\n",
        "    'Betacoronavirus_1',\n",
        "    'Bat_coronavirus_HKU10',\n",
        "    'Avian_coronavirus',\n",
        "    'Porcine_epidemic_diarrhea_virus',\n",
        "    'Alphacoronavirus_1',\n",
        "    'Coronavirus_HKU15',\n",
        "    'Murine_coronavirus',\n",
        "    'Rousettus_bat_coronavirus_HKU9'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8taYUELQknMf"
      },
      "outputs": [],
      "source": [
        "# Iterate over all viruses and calculate color threshold for each virus\n",
        "color_thresholds_dnabert = {}\n",
        "\n",
        "for virus_name in viruses:\n",
        "    file_path_rowl = f'/content/row_linkage_matrices_dnabert/dnabert_rowl_{virus_name}.npy'\n",
        "    row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "    color_threshold = optimal_dendrogram_threshold_and_clusters(row_linkage_matrix, 5)\n",
        "    color_thresholds_dnabert[virus_name] = color_threshold\n",
        "save_json(color_thresholds_dnabert, 'color_thresholds_dnabert.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39XwuweVyiy3"
      },
      "outputs": [],
      "source": [
        "# Iterate over all viruses and calculate color threshold for each virus\n",
        "color_thresholds_zhunt = {}\n",
        "\n",
        "for virus_name in viruses:\n",
        "    file_path_rowl = f'/content/row_linkage_matrices_zhunt/zhunt_rowl_{virus_name}.npy'\n",
        "    row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "    color_threshold = optimal_dendrogram_threshold_and_clusters(row_linkage_matrix, 4)\n",
        "    color_thresholds_zhunt[virus_name] = color_threshold\n",
        "save_json(color_thresholds_zhunt, 'color_thresholds_zhunt.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLLCbzQCMu5y"
      },
      "source": [
        "## Данные по характеристике участков Z-РНК и геномных последовательностей"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C04jf3zQNHHC"
      },
      "outputs": [],
      "source": [
        "# # Create a dataframe with data from genbank files\n",
        "# files = list_files('/content/species_gb_files')\n",
        "# meta_data_all_species = extract_genbank_data(files)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "meta_data_all_species = pd.read_csv('meta_data_all_species.csv', index_col=0)\n",
        "meta_data_all_species = meta_data_all_species.drop('Clusters', axis=1)"
      ],
      "metadata": {
        "id": "gX2FREoePn3W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sars2_meta = pd.read_csv('sars2_meta_10k.csv', index_col=0)\n",
        "sars2_meta['virus_species_from_file'] = 'Severe_acute_respiratory_syndrome_related_coronavirus_2'"
      ],
      "metadata": {
        "id": "3XXaogxjS-T3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZD3vXSGfNXtH"
      },
      "outputs": [],
      "source": [
        "# Add Z-RNA intervals for all species information to the metadata dataframe\n",
        "\n",
        "# Combine DNABERT prediction files into one dictionary\n",
        "combined_dict_dnabert = {}\n",
        "for file_path in list_files('/content/Coronaviridae strains predictions json/species_json_dnabert'):\n",
        "    with open(file_path, 'r') as f:\n",
        "        combined_dict_dnabert.update(json.load(f))\n",
        "\n",
        "# Update the metadata dataframe with DNABERT information\n",
        "meta_data_all_species['ZDNABERT intervals total length'] = meta_data_all_species['accession_with_version'].apply(\n",
        "    lambda x: sum_intervals(combined_dict_dnabert.get(x))\n",
        ")\n",
        "meta_data_all_species['ZDNABERT intervals number'] = meta_data_all_species['accession_with_version'].apply(\n",
        "    lambda x: len(combined_dict_dnabert.get(x))\n",
        ")\n",
        "\n",
        "# Combine ZHUNT prediction files into one dictionary\n",
        "combined_dict_zhunt = {}\n",
        "for file_path in list_files('/content/Coronaviridae strains predictions json/species_json_zhunt'):\n",
        "    with open(file_path, 'r') as f:\n",
        "        combined_dict_zhunt.update(json.load(f))\n",
        "\n",
        "# Update the metadata dataframe with ZHUNT information\n",
        "meta_data_all_species['ZHUNT intervals total length'] = meta_data_all_species['accession_with_version'].apply(\n",
        "    lambda x: sum_intervals(combined_dict_zhunt.get(x, None))\n",
        ")\n",
        "meta_data_all_species['ZHUNT intervals number'] = meta_data_all_species['accession_with_version'].apply(\n",
        "    lambda x: len(combined_dict_zhunt.get(x, None))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add SARS-CoV-2 Z-RNA intervals for all species information to the metadata dataframe\n",
        "\n",
        "# Combine DNABERT prediction files into one dictionary\n",
        "combined_sars2_dnabert = {}\n",
        "for file_path in list_files('/content/SARS-CoV-2-parts/DNABERT'):\n",
        "    with open(file_path, 'r') as f:\n",
        "        combined_sars2_dnabert.update(json.load(f))\n",
        "\n",
        "# Update the metadata dataframe with DNABERT information\n",
        "sars2_meta['ZDNABERT intervals total length'] = sars2_meta['accession_with_version'].apply(\n",
        "    lambda x: sum_intervals(combined_sars2_dnabert.get(x))\n",
        ")\n",
        "sars2_meta['ZDNABERT intervals number'] = sars2_meta['accession_with_version'].apply(\n",
        "    lambda x: len(combined_sars2_dnabert.get(x))\n",
        ")\n",
        "\n",
        "# Combine ZHUNT prediction files into one dictionary\n",
        "combined_sars2_zhunt = {}\n",
        "for file_path in list_files('/content/SARS-CoV-2-parts/ZHUNT'):\n",
        "    with open(file_path, 'r') as f:\n",
        "        combined_sars2_zhunt.update(json.load(f))\n",
        "\n",
        "# Update the metadata dataframe with ZHUNT information\n",
        "sars2_meta['ZHUNT intervals total length'] = sars2_meta['accession_with_version'].apply(\n",
        "    lambda x: sum_intervals(combined_sars2_zhunt.get(x))\n",
        ")\n",
        "sars2_meta['ZHUNT intervals number'] = sars2_meta['accession_with_version'].apply(\n",
        "    lambda x: len(combined_sars2_zhunt.get(x))\n",
        ")"
      ],
      "metadata": {
        "id": "1UxrqErzU93v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint_meta_df = pd.concat([meta_data_all_species, sars2_meta], ignore_index=True)"
      ],
      "metadata": {
        "id": "ScGnlwzuUP_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint_meta_df['ZDNABERT Mean interval length'] = (\n",
        "    joint_meta_df['ZDNABERT intervals total length'] /\n",
        "    joint_meta_df['ZDNABERT intervals number']\n",
        ")\n",
        "\n",
        "joint_meta_df['ZHUNT Mean interval length'] = (\n",
        "    joint_meta_df['ZHUNT intervals total length'] /\n",
        "    joint_meta_df['ZHUNT intervals number']\n",
        ")"
      ],
      "metadata": {
        "id": "567sQydpd4CC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Update the metadata dataframe with Host information\n",
        "joint_meta_df['Host'] = joint_meta_df['virus_species_from_file'].apply(\n",
        "    lambda x: PREVALENT_HOSTS.get(x, None))"
      ],
      "metadata": {
        "id": "n3P-UtWiXJPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Manually fix some NA values\n",
        "joint_meta_df.loc[joint_meta_df['virus_species_from_file'] == 'Eidolon_bat_coronavirus_C704', 'genus'] = 'Betacoronavirus'\n",
        "joint_meta_df.loc[joint_meta_df['virus_species_from_file'] == 'Myodes_coronavirus_2JL14', 'genus'] = 'Betacoronavirus'"
      ],
      "metadata": {
        "id": "hRWG-F51a2Qw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 'Virus name' column, just for aestetics\n",
        "joint_meta_df['Virus name'] = joint_meta_df['virus_species_from_file'].apply(lambda x: x.replace('_', ' '))"
      ],
      "metadata": {
        "id": "sa7NnMmwYT4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pmGjGMLzNh6x"
      },
      "outputs": [],
      "source": [
        "joint_meta_df.to_csv('joint_meta_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "joint_meta_df = pd.read_csv('joint_meta_df.csv')"
      ],
      "metadata": {
        "id": "YP1hEVv_hRem"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "joint_meta_df = remove_outliers(joint_meta_df, ['ZDNABERT intervals total length'], multiplier = 15)\n",
        "#joint_meta_df = remove_outliers(joint_meta_df, ['GC content'], multiplier = 10)"
      ],
      "metadata": {
        "id": "s22RzUR9c2S6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Визуализация распределения характеристик геномной последовательности в зависимости от вида вируса (с указанием основного хозяина вируса)"
      ],
      "metadata": {
        "id": "za_gPJH6fWjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'Sequence length',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "Np3raqsjfq0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'GC content',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "PhF0d2XIgOEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'GC skew',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "SGtbn5i8ghCg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Визуализация распределения характеристик участков Z-РНК в зависимости от вида вируса (с указанием основного хозяина вируса)"
      ],
      "metadata": {
        "id": "ixyCw_oueyKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'ZDNABERT intervals total length',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "Mn2_27S_Xv-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'ZHUNT intervals total length',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "h9AkoZH2c2x-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'ZDNABERT intervals number',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "7TxzYL7XdKGK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'ZHUNT intervals number',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "PGM3nrf9dcOP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'ZDNABERT Mean interval length',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "IQ3215xieVX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "draw_boxplot_species(\n",
        "        data_subset = joint_meta_df,\n",
        "        category_label = 'Virus name',\n",
        "        value_label = 'ZHUNT Mean interval length',\n",
        "        figure_size = (30, 10),\n",
        "        text_position = (0.006, 0.46),\n",
        "        legend_location = 'upper left'\n",
        "        )"
      ],
      "metadata": {
        "id": "v3qnmRgZedVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQf3ylhllQ1K"
      },
      "source": [
        "# Визуализация взаимного расположения участков Z-РНК предсказанных ZDNABERT и ZHUNT\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "84NxhW9PaMau"
      },
      "outputs": [],
      "source": [
        "genomic_regions_sars = extract_regions_from_genbank_id('NC_004718.3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBrxzaudaoV0"
      },
      "outputs": [],
      "source": [
        "genomic_regions_sars = [\n",
        "    (\"5'UTR\", 1, 264), ('nsp1', 265, 804), ('nsp2', 805, 2718),\n",
        "    ('nsp3', 2719, 8484), ('nsp4', 8485, 9984), ('nsp5', 9985, 10902),\n",
        "    ('nsp6', 10903, 11772), ('nsp7', 11773, 12021), ('nsp8', 12022, 12615),\n",
        "    ('nsp9', 12616, 12954), ('nsp10', 12955, 13371), ('nsp11', 13372, 13394),\n",
        "    ('nsp12', 13372, 16166), ('nsp13', 16167, 17969), ('nsp14', 17970, 19550),\n",
        "    ('nsp15', 19551, 20588), ('nsp16', 20589, 21482), ('S', 21492, 25259),\n",
        "    ('ORF3a', 25268, 26092), ('ORF3b', 25689, 26153), ('E', 26117, 26347),\n",
        "    ('M', 26398, 27063), ('ORF6', 27074, 27265), ('ORF7a', 27273, 27641),\n",
        "    ('ORF7b', 27638, 27772), ('ORF8a', 27779, 27898), ('ORF8b', 27864, 28118),\n",
        "    ('N', 28120, 29388), ('ORF9b', 28130, 28426), ('ORF9a', 28583, 28795),\n",
        "    (\"3'UTR\", 28796, 29751)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bugILY5WaIki"
      },
      "outputs": [],
      "source": [
        "# SARS-CoV\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Severe_acute_respiratory_syndrome_related_coronavirus'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(\"SARS\", sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_sars, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uPlGqjjFd_2X"
      },
      "outputs": [],
      "source": [
        "genomic_regions_MERS = extract_regions_from_genbank_id('NC_019843.3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7S-n_p5EeJsE"
      },
      "outputs": [],
      "source": [
        "genomic_regions_MERS = [\n",
        "    (\"5'UTR\", 1, 278), ('nsp1', 279, 857), ('nsp2', 858, 2837),\n",
        "    ('nsp3', 2838, 8498), ('nsp4', 8499, 10019), ('nsp5', 10020, 10937),\n",
        "    ('nsp6', 10938, 11813), ('nsp7', 11814, 12062), ('nsp8', 12063, 12659),\n",
        "    ('nsp9', 12660, 12989), ('nsp10', 12990, 13409), ('nsp11', 13410, 16207),\n",
        "    ('nsp12', 16208, 18001), ('nsp13', 18002, 19573), ('nsp14', 19574, 20602),\n",
        "    ('nsp15', 20603, 21511), ('S', 21456, 25517), ('NS3', 25532, 25843),\n",
        "    ('NS4A', 25852, 26181), ('NS4B', 26093, 26833), ('NS5', 26840, 27514),\n",
        "    ('E', 27590, 27838), ('M', 27853, 28512), ('N', 28566, 29807),\n",
        "    ('ORF8b', 28762, 29100), (\"3'UTR\", 29101, 30119)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CU1oZ2y-aIhw"
      },
      "outputs": [],
      "source": [
        "# MERS\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Middle_East_respiratory_syndrome-related_coronavirus'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(\"MERS\", sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_MERS, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JKc9sf4RaIfC"
      },
      "outputs": [],
      "source": [
        "genomic_regions_229E = extract_regions_from_genbank_id('NC_002645.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3jVTzfr5aIb6"
      },
      "outputs": [],
      "source": [
        "genomic_regions_229E = [\n",
        "    (\"5'UTR\", 1, 292), ('leader protein p9', 293, 625), ('p87', 626, 2983),\n",
        "    ('nsp1', 2984, 7744), ('p44', 7745, 9187), ('nsp2', 9188, 10093),\n",
        "    ('nsp3', 10094, 10930), ('nsp4', 10931, 11179), ('nsp5', 11180, 11764),\n",
        "    ('nsp6', 11765, 12091), ('nsp7', 12092, 12496), ('nsp9', 12497, 15276),\n",
        "    ('nsp10', 15277, 17067), ('nsp11', 17068, 18621), ('nsp12', 18622, 19665),\n",
        "    ('nsp13', 19666, 20565), ('S', 20570, 24091), ('4a', 24091, 24492),\n",
        "    ('4bn', 24482, 24748), ('E', 24750, 24983), ('M', 24995, 25672),\n",
        "    ('N', 25686, 26855), (\"3'UTR\", 26856, 27317)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaRRzAoqaIZT"
      },
      "outputs": [],
      "source": [
        "# 229E\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Human_coronavirus_229E'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(\"Human coronavirus 229E\", sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_229E, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ii-Go2g5jwSY"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Human_HKU1 = extract_regions_from_genbank_id('NC_006577.2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBeg7EpXjwPu"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Human_HKU1 = [\n",
        "    (\"5'UTR\", 1, 205), ('Leader protein', 206, 871), ('nsp2', 872, 2632),\n",
        "    ('nsp3', 2633, 8719), ('nsp4', 8720, 10207), ('nsp5', 10208, 11116),\n",
        "    ('nsp6', 11117, 11977), ('nsp7', 11978, 12253), ('nsp8', 12254, 12835),\n",
        "    ('nsp9', 12836, 13165), ('nsp10', 13166, 13576), ('nsp12', 13577, 16359),\n",
        "    ('nsp11', 13577, 13618), ('nsp13', 16360, 18168), ('nsp14', 18169, 19731),\n",
        "    ('nsp15', 19732, 20853), ('nsp16', 20854, 21750), ('HE', 21773, 22933),\n",
        "    ('S', 22942, 27012), ('NS', 27051, 27380), ('E', 27373, 27621),\n",
        "    ('M', 27633, 28304), ('N', 28320, 29645), ('N2', 28342, 28959),\n",
        "    (\"3'UTR\", 28960, 29926)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CiRcLcx_jwM1"
      },
      "outputs": [],
      "source": [
        "# Human_HKU1\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Human_coronavirus_HKU1'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Human_HKU1, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-c7ya-EjwJf"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Human_NL63 = extract_regions_from_genbank_id('NC_005831.2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6beE5FAjwGW"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Human_NL63 = [\n",
        "    (\"5'UTR\", 1, 286), (\"ORF1ab\", 287, 20475), ('S', 20472, 24542),\n",
        "    ('p3', 24542, 25219), ('E', 25200, 25433), ('M', 25442, 26122),\n",
        "    ('N', 26133, 27266), (\"3'UTR\", 27267, 27553)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIPBcVqjjwDd"
      },
      "outputs": [],
      "source": [
        "# Human_NL63\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Human_coronavirus_NL63'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Human_NL63, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O8zst0vi8wTB"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Human_OC43 = extract_regions_from_genbank_id('NC_006213.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZFWgKT8Q8HY2"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Human_OC43 = [\n",
        "    (\"5'UTR\", 1, 209), ('nsp1', 210, 947), ('nsp2', 948, 2762),\n",
        "    ('nsp3', 2763, 8459), ('nsp4', 8460, 9947), ('nsp5', 9948, 10856),\n",
        "    ('nsp6', 10857, 11717), ('nsp7', 11718, 11984), ('nsp8', 11985, 12575),\n",
        "    ('nsp9', 12576, 12905), ('nsp10', 12906, 13316), ('nsp11', 13317, 13358),\n",
        "    ('nsp12', 13317, 16099), ('nsp13', 16100, 17908), ('nsp14', 17909, 19471),\n",
        "    ('nsp15', 19472, 20596), ('nsp16', 20597, 21493), ('NS2', 21506, 22342),\n",
        "    ('HE', 22354, 23628), ('S', 23643, 27704), ('ORF5', 27792, 28121),\n",
        "    ('E', 28108, 28362), ('M', 28377, 29069), ('N', 29079, 30425),\n",
        "    ('I', 29140, 29322), (\"3'UTR\", 30424, 30741)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z8qytUVT7DFT"
      },
      "outputs": [],
      "source": [
        "# Human coronavirus OC43\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Human_coronavirus_OC43'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv('/content/clustered_data_frames_dnabert/dnabert_cls_df_Human_coronavirus_OC43.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Human_OC43, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWf7xSFsjwAV"
      },
      "outputs": [],
      "source": [
        "# Same as for Human coronavirus OC43\n",
        "genomic_regions_Betacoronavirus1 = extract_regions_from_genbank_id('NC_006213.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmn3q67Tjv9K"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Betacoronavirus1 = [\n",
        "    (\"5'UTR\", 1, 209), ('nsp1', 210, 947), ('nsp2', 948, 2762),\n",
        "    ('nsp3', 2763, 8459), ('nsp4', 8460, 9947), ('nsp5', 9948, 10856),\n",
        "    ('nsp6', 10857, 11717), ('nsp7', 11718, 11984), ('nsp8', 11985, 12575),\n",
        "    ('nsp9', 12576, 12905), ('nsp10', 12906, 13316), ('nsp11', 13317, 13358),\n",
        "    ('nsp12', 13317, 16099), ('nsp13', 16100, 17908), ('nsp14', 17909, 19471),\n",
        "    ('nsp15', 19472, 20596), ('nsp16', 20597, 21493), ('NS2', 21506, 22342),\n",
        "    ('HE', 22354, 23628), ('S', 23643, 27704), ('ORF5', 27792, 28121),\n",
        "    ('E', 28108, 28362), ('M', 28377, 29069), ('N', 29079, 30425),\n",
        "    ('I', 29140, 29322), (\"3'UTR\", 30424, 30741)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oe7ep_hMaIWZ"
      },
      "outputs": [],
      "source": [
        "# Betacoronavirus 1\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Betacoronavirus_1'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Betacoronavirus1, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9IAjXj-uyKT"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Bat_HKU10 = extract_regions_from_genbank_id('NC_018871.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6U1r8-pxu-h_"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Bat_HKU10 = [\n",
        "    (\"5'UTR\", 1, 302), (\"ORF1ab\", 303, 20644), ('S', 20641, 24690),\n",
        "    ('NS3', 24690, 25346), ('E', 25375, 25602), ('M', 25608, 26297),\n",
        "    ('N', 26308, 27516), ('NS7a', 27532, 27777), ('NS7b', 27787, 28248),\n",
        "    ('NS7c', 27986, 28216), (\"3'UTR\", 28249, 28494)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1agk2yB2uyH-"
      },
      "outputs": [],
      "source": [
        "# Bat coronavirus HKU10\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Bat_coronavirus_HKU10'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Bat_HKU10, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NU_LJCduyEk"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Avian = extract_regions_from_genbank_id('NC_048214')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "byepGIpauyB_"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Avian = [\n",
        "    (\"5'UTR\", 1, 346), ('nsp1/2', 347, 2356), ('nsp3', 2357, 7225),\n",
        "    ('nsp4', 7226, 8770), ('nsp5', 8771, 9691), ('nsp6', 9692, 10573),\n",
        "    ('nsp7', 10574, 10822), ('nsp8', 10823, 11452), ('nsp9', 11453, 11785),\n",
        "    ('nsp10', 11786, 12220), ('nsp12', 12221, 15081), ('nsp11', 12221, 12286),\n",
        "    ('nsp13', 15082, 16881), ('nsp14', 16882, 18447), ('nsp15', 18448, 19461),\n",
        "    ('nsp16', 19462, 20292), ('S', 20318, 23893), ('3a', 23893, 24066),\n",
        "    ('3b', 24066, 24257), ('3c', 24238, 24519), ('M', 24516, 25223),\n",
        "    ('ORFX', 25224, 25508), ('5a', 25587, 25784), ('5b', 25781, 26029),\n",
        "    ('N', 25972, 27216), ('ORFY', 27246, 27458), (\"3'UTR\", 27459, 27754)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jz-ycK_Jux-i"
      },
      "outputs": [],
      "source": [
        "# Avian coronavirus\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Avian_coronavirus'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Avian, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9gURS1Ec7TT"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Porcine_ep = extract_regions_from_genbank_id('KJ184549.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BhEYlXAAc7Qw"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Porcine_ep = [\n",
        "    (\"5'UTR\", 1, 292), ('nsp1', 293, 622), ('nsp2', 623, 2872),\n",
        "    ('nsp3', 2873, 7840), ('nsp4', 7841, 9265), ('nsp4', 7841, 9283),\n",
        "    ('nsp5', 9284, 10189), ('nsp6', 10190, 11029), ('nsp7', 11030, 11281),\n",
        "    ('nsp8', 11282, 11863), ('nsp9', 11864, 12187), ('nsp10', 12188, 12592),\n",
        "    ('nsp11', 12593, 12643), ('nsp12', 12593, 15372), ('nsp13', 15373, 17163),\n",
        "    ('nsp14', 17164, 18714), ('nsp15', 18715, 19731), ('nsp16', 19732, 20634),\n",
        "    ('S', 20634, 24794), ('ORF3', 24794, 25468), ('E', 25449, 25679),\n",
        "    ('M', 25687, 26367), ('N', 26379, 27704), (\"3'UTR\", 27705, 28038)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vm13CX5Mc7Ns"
      },
      "outputs": [],
      "source": [
        "# Porcine epidemic diarrhea virus\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Porcine_epidemic_diarrhea_virus'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Porcine_ep, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fnwfZdXZc7LB"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Alphacoronavirus_1 = extract_regions_from_genbank_id('MT239439.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MS5ZYGutux7p"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Alphacoronavirus_1 = [\n",
        "    (\"5'UTR\", 1, 309), ('nsp1', 310, 639), ('nsp2', 640, 2946),\n",
        "    ('nsp3', 2947, 7515), ('nsp4', 7516, 8985), ('nsp5', 8986, 9891),\n",
        "    ('nsp6', 9892, 10773), ('nsp7', 10774, 11022), ('nsp8', 11023, 11607),\n",
        "    ('nsp9', 11608, 11940), ('nsp10', 11941, 12345), ('nsp11', 12346, 12402),\n",
        "    ('nsp12', 12403, 15131), ('nsp13', 15132, 16928), ('nsp14', 16929, 18485),\n",
        "    ('nsp15', 18486, 19502), ('nsp16', 19503, 20402), ('S', 20402, 24766),\n",
        "    ('3a', 24851, 25066), ('3b', 25011, 25226), ('E', 25935, 26183),\n",
        "    ('M', 26194, 26982), ('N', 26995, 28128), ('7a', 28133, 28438),\n",
        "    ('7b', 28443, 29063), (\"3'UTR\", 29063, 29347)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5PtMdJCsY97"
      },
      "outputs": [],
      "source": [
        "# Alphacoronavirus 1\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Alphacoronavirus_1'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Alphacoronavirus_1, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-4AXcPs9sY6e"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Coronavirus_HKU15 = extract_regions_from_genbank_id('NC_039208.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tAt1YF57gvuW"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Coronavirus_HKU15 = [\n",
        "    (\"5'UTR\", 1, 532), (\"ORF1ab\", 533, 19316), ('S', 19317, 22796),\n",
        "    ('E', 22797, 23041), ('M', 23042, 23687), ('NS6 protein', 23688, 23971),\n",
        "    ('N', 23987, 24085), ('NS7 protein', 24086, 25020), (\"3'UTR\", 25021, 25425)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "el1ukHHysY3t"
      },
      "outputs": [],
      "source": [
        "# Coronavirus HKU15\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Coronavirus_HKU15'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Coronavirus_HKU15, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "av02WhcMiLci"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Murine_coronavirus = extract_regions_from_genbank_id('NC_001846.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9LHnncxiii1t"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Murine_coronavirus = [\n",
        "    (\"5'UTR\", 1, 209), ('nsp1', 210, 950), ('nsp2', 951, 2705),\n",
        "    ('nsp3', 2706, 9632), ('nsp4', 9633, 10208), ('nsp5', 10209, 11117),\n",
        "    ('nsp6', 11118, 11978), ('nsp7', 11979, 12245), ('nsp8', 12246, 12836),\n",
        "    ('nsp9', 12837, 13166), ('nsp10', 13167, 13577), ('nsp11', 13578, 13619),\n",
        "    ('nsp12', 13578, 16360), ('nsp13', 16361, 18160), (\"nsp14\", 18161, 19723),\n",
        "    ('nsp15', 19724, 20845), (\"nsp16\", 20846, 21742), ('NS2a', 21771, 22556),\n",
        "    ('E2', 23929, 27903), ('E', 28706, 28957), ('E1', 28968, 29654),\n",
        "    ('N', 29669, 31033), ('I protein', 29733, 30356), (\"3'UTR\", 31032, 31357)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jK94UCXriLZX"
      },
      "outputs": [],
      "source": [
        "# Murine coronavirus\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Murine_coronavirus'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Murine_coronavirus, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aOA5T1JYiLWZ"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Rousettus_HKU9 = extract_regions_from_genbank_id('NC_009021.1')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FaBD2QSiiLTR"
      },
      "outputs": [],
      "source": [
        "genomic_regions_Rousettus_HKU9 = [\n",
        "    (\"5'UTR\", 1, 228), ('nsp1', 229, 753), ('nsp2', 754, 2544),\n",
        "    ('nsp3', 2545, 8055), ('nsp4', 8056, 9537), ('nsp5', 9538, 10455),\n",
        "    ('nsp6', 10456, 11325), ('nsp7', 11326, 11574), ('nsp8', 11575, 12174),\n",
        "    ('nsp9', 12175, 12510), ('nsp10', 12511, 12927), ('nsp11', 12928, 15722),\n",
        "    ('nsp12', 15723, 17525), ('nsp13', 17526, 19115), ('nsp14', 19116, 20126),\n",
        "    (\"nsp15\", 20127, 21017), ('S', 20974, 24798), ('NS3', 24795, 25457),\n",
        "    ('E', 25457, 25696), ('M', 25689, 26357), ('N', 26419, 27825),\n",
        "    ('NS7a', 27869, 28426), ('NS7b', 28433, 28882), (\"3'UTR\", 28883, 29114)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XIhes5X7sY0Q"
      },
      "outputs": [],
      "source": [
        "# Rousettus bat coronavirus HKU9\n",
        "\n",
        "pref = '/content/Coronaviridae strains predictions json/species_json_'\n",
        "\n",
        "virus = 'Rousettus_bat_coronavirus_HKU9'\n",
        "\n",
        "dnabert_intervals = read_json_file(f\"{pref}dnabert/zdnabert_{virus}.json\")\n",
        "zhunt_intervals = read_json_file(f\"{pref}zhunt/zhunt_{virus}.json\")\n",
        "\n",
        "dnabert_cls_df = pd.read_csv(f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus}.csv')\n",
        "\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(virus.replace('_', ' '), sorted_keys,\n",
        "                 dnabert_intervals,\n",
        "                 zhunt_intervals,\n",
        "                 genomic_regions_Rousettus_HKU9, colors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-zjsnBqOz23"
      },
      "source": [
        "# Визуализация кластеров участков Z-РНК предсказанных ZDNABERT и ZHUNT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TY08Pbg_-a0i"
      },
      "outputs": [],
      "source": [
        "viruses = [\n",
        "    'Severe_acute_respiratory_syndrome_related_coronavirus',\n",
        "    'Middle_East_respiratory_syndrome-related_coronavirus',\n",
        "    'Human_coronavirus_229E',\n",
        "    'Human_coronavirus_HKU1',\n",
        "    'Human_coronavirus_NL63',\n",
        "    'Human_coronavirus_OC43',\n",
        "    'Betacoronavirus_1',\n",
        "    'Bat_coronavirus_HKU10',\n",
        "    'Avian_coronavirus',\n",
        "    'Porcine_epidemic_diarrhea_virus',\n",
        "    'Alphacoronavirus_1',\n",
        "    'Coronavirus_HKU15',\n",
        "    'Murine_coronavirus',\n",
        "    'Rousettus_bat_coronavirus_HKU9'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7mdBUm2QzZ3"
      },
      "outputs": [],
      "source": [
        "# Suppress the specific warning from scipy\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"scipy.cluster.hierarchy\")\n",
        "\n",
        "# Iterate over all viruses and plot clusters dendrogram and heatmap for each virus\n",
        "\n",
        "for virus_name in viruses:\n",
        "    try:\n",
        "        file_path_df = f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus_name}.csv'\n",
        "        file_path_rowl = f'/content/row_linkage_matrices_dnabert/dnabert_rowl_{virus_name}.npy'\n",
        "\n",
        "        clustered_dataframe = pd.read_csv(file_path_df, index_col=0)\n",
        "        row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "\n",
        "        title = f\"{virus_name.replace('_', ' ')} (ZDNABERT)\"\n",
        "\n",
        "        color_threshold = color_thresholds_dnabert[virus_name]\n",
        "\n",
        "        plot_heatmap_with_dendrogram(clustered_dataframe, row_linkage_matrix, color_threshold,\n",
        "                                     title=title, figsize=(10, 10), save_figure=True, file_name=virus_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {virus_name}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d23uBWnrxiXZ"
      },
      "outputs": [],
      "source": [
        "# Iterate over all viruses and plot clusters dendrogram and heatmap for each virus\n",
        "\n",
        "for virus_name in viruses:\n",
        "    try:\n",
        "        file_path_df = f'/content/clustered_data_frames_zhunt/zhunt_cls_df_{virus_name}.csv'\n",
        "        file_path_rowl = f'/content/row_linkage_matrices_zhunt/zhunt_rowl_{virus_name}.npy'\n",
        "\n",
        "        clustered_dataframe = pd.read_csv(file_path_df, index_col=0)\n",
        "        row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "\n",
        "        title = f\"{virus_name.replace('_', ' ')} (zhunt)\"\n",
        "\n",
        "        color_threshold = color_thresholds_zhunt[virus_name]\n",
        "\n",
        "        plot_heatmap_with_dendrogram(clustered_dataframe, row_linkage_matrix, color_threshold,\n",
        "                                     title=title, figsize=(10, 10), save_figure=True, file_name=virus_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing {virus_name}: {e}\")\n",
        "        continue"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09qwH05EJGeG"
      },
      "source": [
        "# Анализ корреляций"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Lf5BAsMKXJh"
      },
      "outputs": [],
      "source": [
        "meta_data_all_species = pd.read_csv('meta_data_all_species.csv', index_col=0)\n",
        "meta_data_all_species = meta_data_all_species.drop(['Melting temperature', 'Clusters'], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N414an4HRygD"
      },
      "outputs": [],
      "source": [
        "meta_data_all_species.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-L5rVBVGR8ya"
      },
      "outputs": [],
      "source": [
        "column_names = [\n",
        "    'GC content', 'GC skew', 'ZDNABERT intervals total length',\n",
        "    'ZHUNT intervals total length', 'Collection date'\n",
        "]\n",
        "\n",
        "columns_with_outliers = [\n",
        "    'ZDNABERT intervals total length', 'GC content', 'GC skew', 'genus'\n",
        "]\n",
        "\n",
        "meta_data_all_species = remove_outliers(\n",
        "    meta_data_all_species,\n",
        "    column_names=['ZDNABERT intervals total length'],\n",
        "    multiplier=3\n",
        ")\n",
        "\n",
        "# Convert 'datetime' column to datetime objects\n",
        "meta_data_all_species['datetime'] = pd.to_datetime(meta_data_all_species['datetime'])\n",
        "\n",
        "# Only convert non-NaT values to ordinal, leave NaT values as NaN\n",
        "meta_data_all_species['Collection date'] = meta_data_all_species['datetime'].apply(\n",
        "    lambda x: x.toordinal() if pd.notna(x) else np.nan\n",
        ")\n",
        "\n",
        "meta_data_all_species.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNKKtetx-zGG"
      },
      "outputs": [],
      "source": [
        "# Compute the correlation matrix\n",
        "corr = meta_data_all_species.corr(numeric_only=True)\n",
        "\n",
        "# Generate a mask for the upper triangle (optional)\n",
        "mask = None\n",
        "upper_triangle = True\n",
        "if upper_triangle:\n",
        "    mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "\n",
        "# Set up the matplotlib figure\n",
        "plt.figure(figsize=(10, 8))\n",
        "\n",
        "# Generate the heatmap\n",
        "sns.heatmap(corr, mask=mask, cmap=\"coolwarm\", vmax=1, vmin=-1, center=0, annot=True);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGmdEoCnSSQe"
      },
      "outputs": [],
      "source": [
        "# Set the font scale and markers\n",
        "sns.set_context(\"notebook\", font_scale=0.6)\n",
        "markers = [\"x\", \"+\", \"1\", \"2\", \"3\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtfMiOzCc-uI"
      },
      "outputs": [],
      "source": [
        "selected_columns_seq_prop = ['Sequence length', 'GC content', 'GC skew', 'genus']\n",
        "sns.pairplot(meta_data_all_species[selected_columns_seq_prop], diag_kind=\"kde\", hue=\"genus\", markers=markers);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPN7V7lolxEB"
      },
      "outputs": [],
      "source": [
        "x_vars = ['Sequence length', 'GC content', 'GC skew']\n",
        "y_vars = ['ZDNABERT intervals total length']\n",
        "sns.pairplot(meta_data_all_species, x_vars=x_vars, y_vars=y_vars, diag_kind=\"kde\", hue=\"genus\", markers=markers);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qc1eaNkLcHMr"
      },
      "outputs": [],
      "source": [
        "x_vars = ['Sequence length', 'GC content', 'GC skew']\n",
        "y_vars = ['ZDNABERT intervals number']\n",
        "sns.pairplot(meta_data_all_species, x_vars=x_vars, y_vars=y_vars, diag_kind=\"kde\", hue=\"genus\", markers=markers);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEmrTDWWmDTx"
      },
      "outputs": [],
      "source": [
        "x_vars = ['Sequence length', 'GC content', 'GC skew']\n",
        "y_vars = ['ZHUNT intervals total length']\n",
        "sns.pairplot(meta_data_all_species, x_vars=x_vars, y_vars=y_vars, diag_kind=\"kde\", hue=\"genus\", markers=markers);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_-6SnDtjmOkX"
      },
      "outputs": [],
      "source": [
        "x_vars = ['Sequence length', 'GC content', 'GC skew']\n",
        "y_vars = ['ZHUNT intervals number']\n",
        "sns.pairplot(meta_data_all_species, x_vars=x_vars, y_vars=y_vars, diag_kind=\"kde\", hue=\"genus\", markers=markers);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oMhP0hMZcwuL"
      },
      "outputs": [],
      "source": [
        "z = [\n",
        " 'ZDNABERT intervals total length',\n",
        " 'ZDNABERT intervals number',\n",
        " 'ZHUNT intervals total length',\n",
        " 'ZHUNT intervals number', 'genus'\n",
        " ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZnLbEdYhaPb-"
      },
      "outputs": [],
      "source": [
        "sns.pairplot(meta_data_all_species[z], diag_kind=\"kde\", hue=\"genus\", markers=markers);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Upnx9IXPxi4"
      },
      "source": [
        "# Регрессионный анализ зависимости суммарной длины участков Z-РНК от времени сбора образца вируса"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TWG65RbZWEKZ"
      },
      "outputs": [],
      "source": [
        "meta_data_all_species = pd.read_csv('meta_data_all_species.csv', index_col=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uuESsDfsZOK4"
      },
      "outputs": [],
      "source": [
        "viruses = [\n",
        "    'Severe_acute_respiratory_syndrome_related_coronavirus',\n",
        "    'Middle_East_respiratory_syndrome-related_coronavirus',\n",
        "    'Human_coronavirus_229E',\n",
        "    'Human_coronavirus_HKU1',\n",
        "    'Human_coronavirus_NL63',\n",
        "    'Human_coronavirus_OC43',\n",
        "    'Betacoronavirus_1',\n",
        "    'Bat_coronavirus_HKU10',\n",
        "    'Avian_coronavirus',\n",
        "    'Porcine_epidemic_diarrhea_virus',\n",
        "    'Alphacoronavirus_1',\n",
        "    'Coronavirus_HKU15',\n",
        "    'Murine_coronavirus',\n",
        "    'Rousettus_bat_coronavirus_HKU9'\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fHQ--zISZEU-"
      },
      "outputs": [],
      "source": [
        "# Iterate over all viruses and plot intervals total length vs date for each virus ZDNABERT predictions\n",
        "\n",
        "for virus_name in viruses:\n",
        "    virus = virus_name.replace('_', ' ')\n",
        "    file_path_df = f'/content/clustered_data_frames_dnabert/dnabert_cls_df_{virus_name}.csv'\n",
        "    file_path_rowl = f'/content/row_linkage_matrices_dnabert/dnabert_rowl_{virus_name}.npy'\n",
        "\n",
        "    clustered_dataframe = pd.read_csv(file_path_df, index_col=0)\n",
        "    row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "    color_threshold = color_thresholds_dnabert[virus_name]\n",
        "\n",
        "    dendro = plot_time_length_regression(meta_df=meta_data_all_species,\n",
        "                            virus_name=virus_name,\n",
        "                            clustered_dataframe=clustered_dataframe,\n",
        "                            row_linkage_matrix=row_linkage_matrix,\n",
        "                            color_threshold = color_threshold,\n",
        "                            point_size = 20,\n",
        "                            intervals_column='ZDNABERT intervals total length',\n",
        "                            title=f'{virus}\\nZDNABERT predictions',\n",
        "                            figsize=(15, 10), remove_outliers=(True, 3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZU_3j5juzjYq"
      },
      "outputs": [],
      "source": [
        "# Iterate over all viruses and plot intervals total length vs date for each virus ZHUNT predictions\n",
        "\n",
        "for virus_name in viruses:\n",
        "    virus = virus_name.replace('_', ' ')\n",
        "    file_path_df = f'/content/clustered_data_frames_zhunt/zhunt_cls_df_{virus_name}.csv'\n",
        "    file_path_rowl = f'/content/row_linkage_matrices_zhunt/zhunt_rowl_{virus_name}.npy'\n",
        "\n",
        "    clustered_dataframe = pd.read_csv(file_path_df, index_col=0)\n",
        "    row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "    color_threshold = color_thresholds_zhunt[virus_name]\n",
        "\n",
        "    dendro = plot_time_length_regression(meta_df=meta_data_all_species,\n",
        "                            virus_name=virus_name,\n",
        "                            clustered_dataframe=clustered_dataframe,\n",
        "                            row_linkage_matrix=row_linkage_matrix,\n",
        "                            color_threshold = color_threshold,\n",
        "                            point_size = 20,\n",
        "                            intervals_column='ZHUNT intervals total length',\n",
        "                            title=f'{virus}\\nZHUNT predictions',\n",
        "                            figsize=(15, 10), remove_outliers=(True, 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ne1e3xcWtad"
      },
      "source": [
        "# Анализ участков Z-РНК SARS-CoV-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVw3VgtFdcnl"
      },
      "outputs": [],
      "source": [
        "sars2_meta = extract_genbank_data(['/content/SARS-CoV-2-10k-new.gbk'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xu2_nifF0IzT"
      },
      "outputs": [],
      "source": [
        "sars2_meta = pd.read_csv('sars2_meta_10k.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjKEC8qOlDac"
      },
      "outputs": [],
      "source": [
        "# Combine DNABERT prediction files into one dictionary\n",
        "combined_sars2_dnabert = {}\n",
        "for file_path in list_files('/content/SARS-CoV-2-parts/DNABERT'):\n",
        "    with open(file_path, 'r') as f:\n",
        "        combined_sars2_dnabert.update(json.load(f))\n",
        "\n",
        "# Update the metadata dataframe with DNABERT information\n",
        "sars2_meta['ZDNABERT intervals total length'] = sars2_meta['accession_with_version'].apply(\n",
        "    lambda x: sum_intervals(combined_sars2_dnabert.get(x))\n",
        ")\n",
        "sars2_meta['ZDNABERT intervals number'] = sars2_meta['accession_with_version'].apply(\n",
        "    lambda x: len(combined_sars2_dnabert.get(x))\n",
        ")\n",
        "\n",
        "# Combine ZHUNT prediction files into one dictionary\n",
        "combined_sars2_zhunt = {}\n",
        "for file_path in list_files('/content/SARS-CoV-2-parts/ZHUNT'):\n",
        "    with open(file_path, 'r') as f:\n",
        "        combined_sars2_zhunt.update(json.load(f))\n",
        "\n",
        "# Update the metadata dataframe with ZHUNT information\n",
        "sars2_meta['ZHUNT intervals total length'] = sars2_meta['accession_with_version'].apply(\n",
        "    lambda x: sum_intervals(combined_sars2_zhunt.get(x))\n",
        ")\n",
        "sars2_meta['ZHUNT intervals number'] = sars2_meta['accession_with_version'].apply(\n",
        "    lambda x: len(combined_sars2_zhunt.get(x))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J4ABp1OzixB6"
      },
      "outputs": [],
      "source": [
        "sars2_meta.to_csv('sars2_meta_10k.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "simGzVfJYfLd"
      },
      "outputs": [],
      "source": [
        "!mkdir SARS-CoV-2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r2EVCBHNobXj"
      },
      "outputs": [],
      "source": [
        "dnabert_cls_df, dnabert_rowl = create_clustered_dataframe(\n",
        "    read_json_file('/content/SARS-CoV-2-parts/DNABERT/SARS-CoV-2-part-0_dnabert.json')\n",
        ")\n",
        "dnabert_cls_df.to_csv('/content/SARS-CoV-2/dnabert_cls_df_SARS-CoV-2_0.csv')\n",
        "np.save(\"/content/SARS-CoV-2/dnabert_rowl_SARS-CoV-2_0.npy\", dnabert_rowl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m6YnoDWVjvda"
      },
      "outputs": [],
      "source": [
        "dnabert_cls_df, dnabert_rowl = create_clustered_dataframe(\n",
        "    read_json_file('/content/SARS-CoV-2-parts/DNABERT/SARS-CoV-2-part-0_dnabert.json')\n",
        ")\n",
        "dnabert_cls_df.to_csv('/content/SARS-CoV-2/zhunt_cls_df_SARS-CoV-2_0.csv')\n",
        "np.save(\"/content/SARS-CoV-2/zhunt_rowl_SARS-CoV-2_0.npy\", dnabert_rowl)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly_1-KwTqeMc"
      },
      "outputs": [],
      "source": [
        "# Calculate color threshold for dnabert predictions\n",
        "color_thresholds_sars2_dnabert = {}\n",
        "\n",
        "file_path_rowl = \"/content/SARS-CoV-2/dnabert_rowl_SARS-CoV-2_0.npy\"\n",
        "row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "color_threshold = optimal_dendrogram_threshold_and_clusters(row_linkage_matrix, 5)\n",
        "color_thresholds_sars2_dnabert['SARS-CoV-2'] = color_threshold\n",
        "save_json(color_thresholds_sars2_dnabert, 'color_thresholds_sars2_dnabert.json')\n",
        "\n",
        "# Calculate color threshold for zhunt predictions\n",
        "color_thresholds_sars2_zhunt = {}\n",
        "\n",
        "file_path_rowl = \"/content/SARS-CoV-2/zhunt_rowl_SARS-CoV-2_0.npy\"\n",
        "row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "color_threshold = optimal_dendrogram_threshold_and_clusters(row_linkage_matrix, 5)\n",
        "color_thresholds_sars2_zhunt['SARS-CoV-2'] = color_threshold\n",
        "save_json(color_thresholds_sars2_zhunt, 'color_thresholds_sars2_zhunt.json')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToLy_z8Ssu2p"
      },
      "outputs": [],
      "source": [
        "genomic_regions_sars2 = extract_regions_from_genbank_id('NC_045512.2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taB_gPASuaNy"
      },
      "outputs": [],
      "source": [
        "genomic_regions_sars2 = [\n",
        "    (\"5'UTR\", 1, 265), ('nsp1', 266, 805), ('nsp2', 806, 2719),\n",
        "    ('nsp3', 2720, 8554), ('nsp4', 8555, 10054), ('nsp5', 10055, 10972),\n",
        "    ('nsp6', 10973, 11842), ('nsp7', 11843, 12091), ('nsp8', 12092, 12685),\n",
        "    ('nsp9', 12686, 13024), ('nsp10', 13025, 13441), ('nsp11', 13442, 13480),\n",
        "    ('nsp12', 13442, 16236), ('nsp13', 16237, 18039), ('nsp14', 18040, 19620),\n",
        "    ('nsp15', 19621, 20658), ('nsp16', 20659, 21552), ('S', 21563, 25384),\n",
        "    ('ORF3a', 25393, 26220), ('E', 26245, 26472), ('M', 26523, 27191),\n",
        "    ('ORF6', 27202, 27387), ('ORF7a', 27394, 27759), ('ORF7b', 27756, 27887),\n",
        "    ('ORF8', 27894, 28259), ('N', 28274, 29533), ('ORF10', 29558, 29674),\n",
        "    (\"3'UTR\", 29675, 29903)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-bQeg5R6sthJ"
      },
      "outputs": [],
      "source": [
        "# SARS-CoV-2\n",
        "virus = 'SARS-CoV-2'\n",
        "\n",
        "dnabert_intervals = read_json_file('/content/SARS-CoV-2-parts/DNABERT/SARS-CoV-2-part-0_dnabert.json')\n",
        "zhunt_intervals = read_json_file('/content/SARS-CoV-2-parts/ZHUNT/SARS-CoV-2-part-0_zhunt.json')\n",
        "sorted_keys = dnabert_cls_df.columns.to_list()\n",
        "\n",
        "plot_zna_regions(\"SARS-CoV-2\", sorted_keys, dnabert_intervals, zhunt_intervals, genomic_regions_sars2, colors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J46IfmxcZ4GW"
      },
      "outputs": [],
      "source": [
        "# Suppress the specific warning from scipy\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"scipy.cluster.hierarchy\")\n",
        "\n",
        "# Plot clusters dendrogram and heatmap for SARS-CoV-2 1k sequences (DNABERT)\n",
        "\n",
        "file_path_df = '/content/SARS-CoV-2/dnabert_cls_df_SARS-CoV-2_0.csv'\n",
        "file_path_rowl = '/content/SARS-CoV-2/dnabert_rowl_SARS-CoV-2_0.npy'\n",
        "\n",
        "clustered_dataframe = pd.read_csv(file_path_df, index_col=0)\n",
        "row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "\n",
        "title = \"SARS-CoV-2 (ZDNABERT)\"\n",
        "\n",
        "color_threshold = color_thresholds_sars2_dnabert['SARS-CoV-2']\n",
        "\n",
        "plot_heatmap_with_dendrogram(clustered_dataframe, row_linkage_matrix, color_threshold,\n",
        "                              title=title, figsize=(10, 10), save_figure=True, file_name='SARS-CoV-2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gFGHEUEtdSYv"
      },
      "outputs": [],
      "source": [
        "# Suppress the specific warning from scipy\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"scipy.cluster.hierarchy\")\n",
        "\n",
        "# Plot clusters dendrogram and heatmap for SARS-CoV-2 1k sequences (ZHUNT)\n",
        "\n",
        "file_path_df = \"/content/SARS-CoV-2/zhunt_cls_df_SARS-CoV-2_0.csv\"\n",
        "file_path_rowl = \"/content/SARS-CoV-2/zhunt_rowl_SARS-CoV-2_0.npy\"\n",
        "\n",
        "clustered_dataframe = pd.read_csv(file_path_df, index_col=0)\n",
        "row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "\n",
        "title = \"SARS-CoV-2 (ZHUNT)\"\n",
        "\n",
        "color_threshold = color_thresholds_sars2_zhunt['SARS-CoV-2']\n",
        "\n",
        "plot_heatmap_with_dendrogram(clustered_dataframe, row_linkage_matrix, color_threshold,\n",
        "                              title=title, figsize=(10, 10), save_figure=True, file_name='SARS-CoV-2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Z-RNA intervals information to the metadata dataframe\n",
        "sars2_0_ids = pd.read_csv('/content/SARS-CoV-2/dnabert_cls_df_SARS-CoV-2_0.csv', index_col=0).columns.to_list()\n",
        "meta_sars2_0 = sars2_meta[sars2_meta['accession_with_version'].isin(sars2_0_ids)]\n",
        "combined_dict_dnabert = read_json_file('/content/SARS-CoV-2-parts/DNABERT/SARS-CoV-2-part-0_dnabert.json')\n",
        "\n",
        "# Update the metadata dataframe with DNABERT information\n",
        "meta_sars2_0['ZDNABERT intervals total length'] = meta_sars2_0['accession_with_version'].apply(\n",
        "    lambda x: sum_intervals(combined_dict_dnabert.get(x))\n",
        ")\n",
        "meta_sars2_0['ZDNABERT intervals number'] = meta_sars2_0['accession_with_version'].apply(\n",
        "    lambda x: len(combined_dict_dnabert.get(x))\n",
        ")\n",
        "\n",
        "combined_dict_zhunt = read_json_file('/content/SARS-CoV-2-parts/ZHUNT/SARS-CoV-2-part-0_zhunt.json')\n",
        "\n",
        "# Update the metadata dataframe with ZHUNT information\n",
        "meta_sars2_0['ZHUNT intervals total length'] = meta_sars2_0['accession_with_version'].apply(\n",
        "    lambda x: sum_intervals(combined_dict_zhunt.get(x, None))\n",
        ")\n",
        "meta_sars2_0['ZHUNT intervals number'] = meta_sars2_0['accession_with_version'].apply(\n",
        "    lambda x: len(combined_dict_zhunt.get(x, None))\n",
        ")"
      ],
      "metadata": {
        "id": "rKIfYjhZ9-6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot intervals total length vs date for ZDNABERT predictions\n",
        "\n",
        "virus = \"SARS-CoV-2\"\n",
        "file_path_df = '/content/SARS-CoV-2/dnabert_cls_df_SARS-CoV-2_0.csv'\n",
        "file_path_rowl = '/content/SARS-CoV-2/dnabert_rowl_SARS-CoV-2_0.npy'\n",
        "\n",
        "clustered_dataframe = pd.read_csv(file_path_df, index_col=0)\n",
        "row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "color_threshold = color_thresholds_sars2_dnabert['SARS-CoV-2']\n",
        "\n",
        "dendro = plot_time_length_regression(meta_df=meta_sars2_0,\n",
        "                        virus_name='SARS-CoV-2-10k-new',\n",
        "                        clustered_dataframe=clustered_dataframe,\n",
        "                        row_linkage_matrix=row_linkage_matrix,\n",
        "                        color_threshold = color_threshold,\n",
        "                        point_size = 10,\n",
        "                        intervals_column='ZDNABERT intervals total length',\n",
        "                        title=f'{virus}\\nZDNABERT predictions',\n",
        "                        figsize=(15, 10), remove_outliers=(True, 3))"
      ],
      "metadata": {
        "id": "Te8Lz_M06nSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot intervals total length vs date for ZHUNT predictions\n",
        "\n",
        "virus = \"SARS-CoV-2\"\n",
        "file_path_df = '/content/SARS-CoV-2/zhunt_cls_df_SARS-CoV-2_0.csv'\n",
        "file_path_rowl = '/content/SARS-CoV-2/zhunt_rowl_SARS-CoV-2_0.npy'\n",
        "\n",
        "clustered_dataframe = pd.read_csv(file_path_df, index_col=0)\n",
        "row_linkage_matrix = np.load(file_path_rowl, allow_pickle=True)\n",
        "color_threshold = color_thresholds_sars2_zhunt['SARS-CoV-2']\n",
        "\n",
        "dendro = plot_time_length_regression(meta_df=meta_sars2_0,\n",
        "                        virus_name='SARS-CoV-2-10k-new',\n",
        "                        clustered_dataframe=clustered_dataframe,\n",
        "                        row_linkage_matrix=row_linkage_matrix,\n",
        "                        color_threshold = color_threshold,\n",
        "                        point_size = 10,\n",
        "                        intervals_column='ZHUNT intervals total length',\n",
        "                        title=f'{virus}\\nZHUNT predictions',\n",
        "                        figsize=(15, 10), remove_outliers=(True, 3))"
      ],
      "metadata": {
        "id": "nqJpVn1VGsYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add WHO lineage information\n",
        "sars2_who_labels = pd.read_csv('/content/SARS-CoV-2-full-genomes-only.csv')\n",
        "print('All complete genome strains count: ', sars2_who_labels.shape[0])\n",
        "\n",
        "sars2_meta = pd.read_csv('/content/sars2_meta_10k.csv')\n",
        "print('analyzed strains count: ', sars2_meta.shape[0])\n",
        "\n",
        "sars2_who_labels_dict = dict(zip(sars2_who_labels['Accession'], sars2_who_labels['Pangolin']))\n",
        "sars2_meta['Pango'] = sars2_meta['accession_with_version'].apply(lambda x: sars2_who_labels_dict.get(x))\n",
        "\n",
        "sars2_meta['WHO'] = sars2_meta['Pango'].apply(lambda x: pango_to_who(x))\n",
        "\n",
        "sars2_meta = remove_outliers(sars2_meta, ['GC content'], multiplier=3)\n",
        "sars2_meta = remove_outliers(sars2_meta, ['ZDNABERT intervals total length'], multiplier=3)\n",
        "sars2_meta = remove_outliers(sars2_meta, ['ZDNABERT intervals number'], multiplier=3)\n",
        "print('Outlier strains count: ', 10000 - len(sars2_meta))"
      ],
      "metadata": {
        "id": "t5xqixoVQ1kJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ZDNABERT intervals total length for WHO linages\n",
        "draw_boxplot_who(\n",
        "        df_subset = sars2_meta,\n",
        "        label = 'WHO',\n",
        "        values = 'ZDNABERT intervals total length',\n",
        "        figsize = (15, 6),\n",
        "        pos = (0.02, 0.02)\n",
        "        )"
      ],
      "metadata": {
        "id": "JoxyfwoXWZgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ZDNABERT intervals number for WHO linages\n",
        "draw_boxplot_who(\n",
        "        df_subset = sars2_meta,\n",
        "        label = 'WHO',\n",
        "        values = 'ZDNABERT intervals number',\n",
        "        figsize = (15, 6),\n",
        "        pos = (0.02, 0.02)\n",
        "        )"
      ],
      "metadata": {
        "id": "TBFFSWNhXWKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ZHUNT intervals total length for WHO linages\n",
        "draw_boxplot_who(\n",
        "        df_subset = sars2_meta,\n",
        "        label = 'WHO',\n",
        "        values = 'ZHUNT intervals total length',\n",
        "        figsize = (15, 6),\n",
        "        pos = (0.02, 0.02)\n",
        "        )"
      ],
      "metadata": {
        "id": "2azLjAG9XQkM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ZHUNT intervals number for WHO linages\n",
        "draw_boxplot_who(\n",
        "        df_subset = sars2_meta,\n",
        "        label = 'WHO',\n",
        "        values = 'ZHUNT intervals number',\n",
        "        figsize = (15, 6),\n",
        "        pos = (0.02, 0.02)\n",
        "        )"
      ],
      "metadata": {
        "id": "EQreKQ-TXgAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot GC content for WHO linages\n",
        "draw_boxplot_who(\n",
        "        df_subset = sars2_meta,\n",
        "        label = 'WHO',\n",
        "        values = 'GC content',\n",
        "        figsize = (15, 6),\n",
        "        pos = (0.02, 0.02)\n",
        "        )"
      ],
      "metadata": {
        "id": "O1ap8_C7XoX5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ZDNABERT intervals total length by collection date\n",
        "plot_time_length_linages_boxplot(sars2_meta, 'ZDNABERT intervals total length', figsize = (20, 8))"
      ],
      "metadata": {
        "id": "ANMOFrnea47i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot ZHUNT intervals total length by collection date\n",
        "plot_time_length_linages_boxplot(sars2_meta, 'ZHUNT intervals total length', figsize = (20, 8))"
      ],
      "metadata": {
        "id": "DxEmfViqdVxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot ZDNABERT intervals total length by collection date for WHO lineages\n",
        "plot_time_length_regression_linages(sars2_meta, 'ZDNABERT intervals total length', figsize = (20, 8))"
      ],
      "metadata": {
        "id": "dT_DJYVPet11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot ZHUNT intervals total length by collection date for WHO lineages\n",
        "plot_time_length_regression_linages(sars2_meta, 'ZHUNT intervals total length', figsize = (20, 8))"
      ],
      "metadata": {
        "id": "Cn3iFhzDeXo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Plot ZDNABERT intervals total length by collection date for Pangolin lineages\n",
        "plot_time_length_regression_linages(sars2_meta,\n",
        "                                    'ZDNABERT intervals total length',\n",
        "                                    category_column='Pango',\n",
        "                                    figsize = (20, 8), show_legend=False)"
      ],
      "metadata": {
        "id": "3It-2YoakDa_"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "l65vm4ZcO7_4",
        "u09GKBtwhzC7",
        "0Upnx9IXPxi4"
      ],
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}